{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/data1/malto/cache'\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = \"/data1/malto/fborra/fig\"\n",
    "BATCH_SIZE = 48\n",
    "NUM_EPOCHS = 8\n",
    "N_GEN = 50\n",
    "base_checkpoint = \"t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_checkpoint)\n",
    "\n",
    "# ds = load_dataset(\"ColumbiaNLP/FLUTE\").shuffle(seed=42)\n",
    "df = pd.read_csv(\"complete_dataset.csv\").fillna(\"\")\n",
    "ds = Dataset.from_pandas(df).shuffle(seed=42)\n",
    "folds = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "splits = folds.split(ds, ds[\"label\"])\n",
    "indexes = [t for t in splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a720d644d32841f3aa3577fa7b2c5685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7534 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from flute_dream import add_combined_cols\n",
    "\n",
    "ds = ds.map(add_combined_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_s1(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis\"])\n",
    "    labels = tokenizer(examples[\"label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s2(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_system_2\"])\n",
    "    labels = tokenizer(examples[\"type_label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s31(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_emotion\"])\n",
    "    labels = tokenizer(examples[\"label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s32(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_motivation\"])\n",
    "    labels = tokenizer(examples[\"label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s33(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_consequence\"])\n",
    "    labels = tokenizer(examples[\"label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s34(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_rot\"])\n",
    "    labels = tokenizer(examples[\"label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s35(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_all_dims\"])\n",
    "    labels = tokenizer(examples[\"label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s41(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis\"])\n",
    "    labels = tokenizer(examples[\"label\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s42(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_label\"])\n",
    "    labels = tokenizer(examples[\"explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s7(examples):\n",
    "    lbl_exp = [\n",
    "        ex[ex.find(\"Explanation: \") :] + ex[: ex.find(\"Explanation: \")]\n",
    "        for ex in examples[\"label_explanation\"]\n",
    "    ]\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis\"])\n",
    "    labels = tokenizer(lbl_exp)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def get_path(name):\n",
    "    return f\"{BASE_DIR}/{name}/{os.listdir(BASE_DIR + '/' + name)[0]}\"\n",
    "\n",
    "\n",
    "def get_prem_hyp(s):\n",
    "    ind = s.find(\"Hypothesis: \")\n",
    "    prem = s[len(\"Premise: \") : ind]\n",
    "    hyp = s[ind + len(\"Hypothesis: \") :]\n",
    "    return [prem, hyp]\n",
    "\n",
    "\n",
    "\"\"\"Class encapsulating the two steps of System 4 (Classify, then Explain)\"\"\"\n",
    "\n",
    "\n",
    "class DREAM_FLUTE_System4:\n",
    "    def __init__(\n",
    "        self, tokenizer=None, model_s41_path=None, model_s42_path=None\n",
    "    ) -> None:\n",
    "        self.tokenizer = (\n",
    "            tokenizer\n",
    "            if tokenizer is not None\n",
    "            else AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "        )\n",
    "        self.model_s41 = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(model_s41_path)\n",
    "            if model_s41_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                \"YoanBOUTE/DREAM-FLUTE-S4-Classify\"\n",
    "            )\n",
    "        )\n",
    "        self.model_s42 = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(model_s42_path)\n",
    "            if model_s42_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                \"YoanBOUTE/DREAM-FLUTE-S4-Explain\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    \"\"\"Expected input for function : \"Premise: ... . Hypothesis: ... . Is there a contradiction or entailment between the premise and hypothesis ?\" \n",
    "    Or list of strings in this format\"\"\"\n",
    "\n",
    "    def prediction_pipeline(self, inputs):\n",
    "        if isinstance(inputs, str):\n",
    "            tok_input = self.tokenizer(inputs, return_tensors=\"pt\").input_ids\n",
    "            output_model_1 = self.model_s41.generate(tok_input, max_new_tokens=100)\n",
    "            decoded_output_model_1 = \"Label: \" + self.tokenizer.decode(\n",
    "                output_model_1[0], skip_special_tokens=True\n",
    "            )\n",
    "            intermediate_input = (\n",
    "                inputs[\n",
    "                    : inputs.find(\n",
    "                        \"Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "                    )\n",
    "                ]\n",
    "                + decoded_output_model_1\n",
    "                + \". What is the explanation of the label associated to the premise and the hypothesis ?\"\n",
    "            )\n",
    "            tok_intermediate_input = self.tokenizer(\n",
    "                intermediate_input, return_tensors=\"pt\"\n",
    "            ).input_ids\n",
    "            output_model_2 = self.model_s42.generate(\n",
    "                tok_intermediate_input, max_new_tokens=100\n",
    "            )\n",
    "\n",
    "            return (\n",
    "                decoded_output_model_1\n",
    "                + \". Explanation: \"\n",
    "                + self.tokenizer.decode(output_model_2[0], skip_special_tokens=True)\n",
    "            )\n",
    "\n",
    "        elif isinstance(inputs, list) and all(\n",
    "            isinstance(input, str) for input in inputs\n",
    "        ):\n",
    "            predictions = []\n",
    "            for input in inputs:\n",
    "                predictions.append(self.prediction_pipeline(input))\n",
    "\n",
    "            return predictions\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"Inputs should be either a list of two strings or a list of lists of two strings\"\n",
    "            )\n",
    "\n",
    "\n",
    "\"\"\"Ensemble class that loads all models from HuggingFace (or from the device if a path to the model is indicated) \n",
    "and implements the ensembling algorithm given in the DREAM-FLUTE paper\"\"\"\n",
    "\n",
    "\n",
    "class DREAM_FLUTE_Ensemble:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer_path=None,\n",
    "        s1_path=None,\n",
    "        s2_path=None,\n",
    "        s3_emo_path=None,\n",
    "        s3_mot_path=None,\n",
    "        s3_cons_path=None,\n",
    "        s3_rot_path=None,\n",
    "        s3_alldims_path=None,\n",
    "        s4_clas_path=None,\n",
    "        s4_exp_path=None,\n",
    "        dream_path=None,\n",
    "    ) -> None:\n",
    "        self.tokenizer = (\n",
    "            AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "            if tokenizer_path is not None\n",
    "            else AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "        )\n",
    "        self.model_s1 = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s1_path)\n",
    "            if s1_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\"YoanBOUTE/DREAM-FLUTE-S1\")\n",
    "        )\n",
    "        self.model_s2 = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s2_path)\n",
    "            if s2_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\"YoanBOUTE/DREAM-FLUTE-S2\")\n",
    "        )\n",
    "        self.model_s3_emo = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s3_emo_path)\n",
    "            if s3_emo_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                \"YoanBOUTE/DREAM-FLUTE-S3-Emotion\"\n",
    "            )\n",
    "        )\n",
    "        self.model_s3_mot = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s3_mot_path)\n",
    "            if s3_mot_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                \"YoanBOUTE/DREAM-FLUTE-S3-Motivation\"\n",
    "            )\n",
    "        )\n",
    "        self.model_s3_cons = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s3_cons_path)\n",
    "            if s3_cons_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                \"YoanBOUTE/DREAM-FLUTE-S3-Consequence\"\n",
    "            )\n",
    "        )\n",
    "        self.model_s3_rot = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s3_rot_path)\n",
    "            if s3_rot_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\"YoanBOUTE/DREAM-FLUTE-S3-ROT\")\n",
    "        )\n",
    "        self.model_s3_alldims = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s3_alldims_path)\n",
    "            if s3_alldims_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                \"YoanBOUTE/DREAM-FLUTE-S3-AllDims\"\n",
    "            )\n",
    "        )\n",
    "        self.model_s4 = DREAM_FLUTE_System4(self.tokenizer, s4_clas_path, s4_exp_path)\n",
    "        self.model_dream = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(dream_path)\n",
    "            if dream_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\"RicoBorra/DREAM-t5-small\")\n",
    "        )\n",
    "\n",
    "    \"\"\"Tokenizes the input, then feeds it to the given model, and decodes the output to have a string as result.\n",
    "    This method is callable for all models except System 4 (Use the method defined in the class of System 4)\"\"\"\n",
    "\n",
    "    def _prediction_pipeline(self, input: str, model) -> str:\n",
    "        tokenized_input = self.tokenizer(input, return_tensors=\"pt\").input_ids\n",
    "        model_output = model.generate(tokenized_input, max_new_tokens=100)\n",
    "        decoded_output = self.tokenizer.decode(\n",
    "            model_output[0], skip_special_tokens=True\n",
    "        )\n",
    "        return decoded_output\n",
    "\n",
    "    \"\"\"Preprocesses the input for each model, then feeds it to the pipeline.\n",
    "    Returns a dictionary of all models' predictions.\"\"\"\n",
    "\n",
    "    def _get_all_predictions(self, input: list):\n",
    "        prem, hyp = input\n",
    "        prem = prem.strip()\n",
    "        hyp = hyp.strip()\n",
    "        if not prem.endswith(\".\"):\n",
    "            prem += \".\"\n",
    "        if not hyp.endswith(\".\"):\n",
    "            hyp += \".\"\n",
    "\n",
    "        predictions = dict()\n",
    "\n",
    "        input_1 = f\"Premise: {prem} Hypothesis: {hyp} Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S1\"] = self._prediction_pipeline(input_1, self.model_s1)\n",
    "\n",
    "        input_2 = f\"Premise: {prem} Hypothesis: {hyp} What is the type of figurative language involved? Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S2\"] = self._prediction_pipeline(input_2, self.model_s2)\n",
    "\n",
    "        # DREAM elaborations for system 3\n",
    "        input_dream_prem = f\"[SITUATION] {prem} [QUERY] \"\n",
    "        input_dream_hyp = f\"[SITUATION] {hyp} [QUERY] \"\n",
    "        prem_elaborations = {\n",
    "            key: self._prediction_pipeline(input_dream_prem + key, self.model_dream)\n",
    "            for key in [\"emotion\", \"motivation\", \"consequence\", \"rot\"]\n",
    "        }\n",
    "        for key, elab in prem_elaborations.items():\n",
    "            elab = elab.strip()\n",
    "            if not elab.endswith(\".\"):\n",
    "                prem_elaborations[key] += \".\"\n",
    "        hyp_elaborations = {\n",
    "            key: self._prediction_pipeline(input_dream_hyp + key, self.model_dream)\n",
    "            for key in [\"emotion\", \"motivation\", \"consequence\", \"rot\"]\n",
    "        }\n",
    "        for key, elab in hyp_elaborations.items():\n",
    "            elab = elab.strip()\n",
    "            if not elab.endswith(\".\"):\n",
    "                hyp_elaborations[key] += \".\"\n",
    "\n",
    "        input_3_emo = f\"Premise: {prem} [Emotion] {prem_elaborations['emotion']} Hypothesis: {hyp} [Emotion] {hyp_elaborations['emotion']} Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S3-emo\"] = self._prediction_pipeline(\n",
    "            input_3_emo, self.model_s3_emo\n",
    "        )\n",
    "\n",
    "        input_3_mot = f\"Premise: {prem} [Motivation] {prem_elaborations['motivation']} Hypothesis: {hyp} [Motivation] {hyp_elaborations['motivation']} Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S3-mot\"] = self._prediction_pipeline(\n",
    "            input_3_mot, self.model_s3_mot\n",
    "        )\n",
    "\n",
    "        input_3_cons = f\"Premise: {prem} [Consequence] {prem_elaborations['consequence']} Hypothesis: {hyp} [Consequence] {hyp_elaborations['consequence']} Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S3-cons\"] = self._prediction_pipeline(\n",
    "            input_3_cons, self.model_s3_cons\n",
    "        )\n",
    "\n",
    "        input_3_rot = f\"Premise: {prem} [Rot] {prem_elaborations['rot']} Hypothesis: {hyp} [Rot] {hyp_elaborations['rot']} Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S3-rot\"] = self._prediction_pipeline(\n",
    "            input_3_rot, self.model_s3_rot\n",
    "        )\n",
    "\n",
    "        input_3_all = f\"Premise: {prem} \"\n",
    "        for key, elab in prem_elaborations.items():\n",
    "            input_3_all += f\"[{key.capitalize()}] {elab} \"\n",
    "        input_3_all += f\"Hypothesis: {hyp} \"\n",
    "        for key, elab in hyp_elaborations.items():\n",
    "            input_3_all += f\"[{key.capitalize()}] {elab} \"\n",
    "        input_3_all += \"Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S3-all\"] = self._prediction_pipeline(\n",
    "            input_3_all, self.model_s3_alldims\n",
    "        )\n",
    "\n",
    "        # The input for system 4 is in the same format as for system 1\n",
    "        predictions[\"S4\"] = self.model_s4.prediction_pipeline(input_1)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    \"\"\"Uses the predictions from each model to compute the final prediction of the ensemble\"\"\"\n",
    "\n",
    "    def _ensemble_algorithm(self, model_outputs):\n",
    "        # Firstly, the label is selected based on the majority between the 5 best models (according to the paper : systems 1, 2, 3-motivation, 3-alldims, 4)\n",
    "        labels = [\n",
    "            model_outputs[key].split(\".\")[0]\n",
    "            for key in [\"S1\", \"S2\", \"S3-mot\", \"S3-all\", \"S4\"]\n",
    "        ]\n",
    "        # Sometimes, it might happen with the small models that the generated label is a mix of words, like 'Contratailment' or 'Endiction'\n",
    "        for label in labels:\n",
    "            if label not in [\"Label: Contradiction\", \"Label: Entailment\"]:\n",
    "                labels.remove(label)\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        ix = np.argmax(counts)\n",
    "        major_label = unique[ix]\n",
    "\n",
    "        # Then, pick the explanation of the first system agreeing with the majority label, following an order indicated in the paper\n",
    "        for key in [\"S3-cons\", \"S3-emo\", \"S2\", \"S3-all\", \"S3-mot\", \"S4\", \"S1\"]:\n",
    "            substrings = model_outputs[key].split(\".\")\n",
    "            label = substrings[0]\n",
    "            explanation = substrings[1]\n",
    "\n",
    "            if label == major_label:\n",
    "                break\n",
    "\n",
    "        return major_label + \".\" + explanation + \".\"\n",
    "\n",
    "    \"\"\"Expected input : [Premise_sentence, hypothesis_sentence] or list of inputs\"\"\"\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        if (\n",
    "            isinstance(inputs, list)\n",
    "            and all(isinstance(input, str) for input in inputs)\n",
    "            and len(inputs) == 2\n",
    "        ):\n",
    "            preds = self._get_all_predictions(inputs)\n",
    "            final_pred = self._ensemble_algorithm(preds)\n",
    "\n",
    "            return final_pred\n",
    "\n",
    "        elif isinstance(inputs, list) and all(\n",
    "            isinstance(input, list) for input in inputs\n",
    "        ):\n",
    "            predictions = []\n",
    "            for input in inputs:\n",
    "                predictions.append(self.predict(input))\n",
    "\n",
    "            return predictions\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"Inputs should be either a list of two strings or a list of lists of two strings\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "operating_modes = [\n",
    "    (\"system_1\", preprocess_dataset_s1),\n",
    "    (\"system_2\", preprocess_dataset_s2),\n",
    "    (\"system_31\", preprocess_dataset_s31),\n",
    "    (\"system_32\", preprocess_dataset_s32),\n",
    "    (\"system_33\", preprocess_dataset_s33),\n",
    "    (\"system_34\", preprocess_dataset_s34),\n",
    "    (\"system_35\", preprocess_dataset_s35),\n",
    "    (\"system_4\", preprocess_dataset_s1),\n",
    "    (\"system_5\", preprocess_dataset_s1),\n",
    "    (\"system_6\", preprocess_dataset_s1),\n",
    "    (\"system_7\", preprocess_dataset_s7),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(name, model, curr_ds):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"{BASE_DIR}/{name}\",\n",
    "        learning_rate=3e-4,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=2 * BATCH_SIZE,\n",
    "        save_total_limit=1,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        report_to=\"none\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        eval_accumulation_steps=1,\n",
    "        logging_steps=1,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=curr_ds[\"train\"],\n",
    "        eval_dataset=curr_ds[\"val\"].select(range(350)),\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': {'system_1': [0.6353626006057131, 0.6418808184651508, 0.6368893212990216, 0.6492920855123814, 0.6466564938196772], 'system_2': [0.6395410100913216, 0.6375092144914072, 0.6376673027978297, 0.6457388758618523, 0.647300237510074], 'system_31': [0.626355555278505, 0.6356330453216885, 0.6291095264502599, 0.6450184585425366, 0.6436652005295087], 'system_32': [0.6371932142491179, 0.641397675246638, 0.6364828751436804, 0.6477715242236362, 0.6435581473896589], 'system_33': [0.6378660489897562, 0.6392807467244879, 0.6360615453917244, 0.6455220666261619, 0.6483284197355579], 'system_34': [0.6307988129771418, 0.6427833303251349, 0.6336232336606692, 0.6462490010619415, 0.6456405508577208], 'system_35': [0.6341961856557128, 0.6430189888364645, 0.6335775754750506, 0.6463285354304195, 0.6427587187656778], 'system_4': [0.5062340995265088, 0.521818155305737, 0.5168288841651111, 0.5238418707432043, 0.5149894127994684], 'system_5': [0.503105247891353, 0.5168062541950713, 0.5077644239985221, 0.5154003524752064], 'system_6': [0.645522294147274, 0.6509578961515939, 0.646718912720274, 0.6572776413883441], 'system_7': [0.6410886725536385, 0.6466373899845665, 0.6415266811645343, 0.6566623483142244]}, 'rouge2': {'system_1': [0.3799781206875058, 0.3885684106342296, 0.38504971266333254, 0.3938426162633386, 0.3874339057864731], 'system_2': [0.38222970469374484, 0.3806427780561566, 0.3888763075009541, 0.39005688912123737, 0.3907885498263465], 'system_31': [0.37008388987613344, 0.3804896322822194, 0.37107984078329964, 0.3874510045800557, 0.38630398897356893], 'system_32': [0.38338399621901714, 0.38703113649518983, 0.3855156484229585, 0.39074869534313705, 0.38543534193629136], 'system_33': [0.37877336356803715, 0.38368442082809906, 0.3799258868701997, 0.38617712522857645, 0.39189171430663994], 'system_34': [0.3706513545170358, 0.385989443728161, 0.3770407772002347, 0.38729292544256205, 0.38493414949981725], 'system_35': [0.3791718899765438, 0.3891698285790503, 0.38069862335929705, 0.38596039885871725, 0.3865191929458037], 'system_4': [0.2818060330658794, 0.29635169072816914, 0.2930705750339089, 0.3026780509823721, 0.2877697286265143], 'system_5': [0.2775921942959165, 0.2925995378889651, 0.28782410339958486, 0.2959403666648639], 'system_6': [0.3945045309095112, 0.4048616845047611, 0.4008219380303524, 0.4078116698102447], 'system_7': [0.38241278174339227, 0.39275145159485286, 0.3865819476620745, 0.4002802359425794]}, 'rougeL': {'system_1': [0.6023834016117641, 0.6107585225024069, 0.6048688383401838, 0.6193039472971151, 0.6117986691783841], 'system_2': [0.6068464719808768, 0.6072451939695136, 0.6066037250799674, 0.6155603108683492, 0.6135966818704379], 'system_31': [0.5927283579893512, 0.6054351631124055, 0.5962931639386194, 0.6163387581372624, 0.6088470404992611], 'system_32': [0.6027513742241442, 0.6094483225763314, 0.6043736168818306, 0.6172492023098091, 0.6091921447983717], 'system_33': [0.6030482269455123, 0.6088491113090367, 0.6042434865728071, 0.6156300268879314, 0.614075997353156], 'system_34': [0.5970578392987896, 0.6112217814658762, 0.602386418194217, 0.6147901018667773, 0.6109343715888967], 'system_35': [0.6009545731578227, 0.6107615876608187, 0.6030400470171788, 0.6151111546315653, 0.6090485560712955], 'system_4': [0.4325611022680163, 0.44965053387358855, 0.4460259778265402, 0.45683967827132776, 0.4402633432754091], 'system_5': [0.43137237135844797, 0.44453449145738805, 0.4377579530440101, 0.448675371837816], 'system_6': [0.6143244955803099, 0.6237333799270951, 0.6187991110411634, 0.6290413466609605], 'system_7': [0.6014973164232106, 0.6093313605841371, 0.6038716991237638, 0.6183863711732847]}, 'rougeLsum': {'system_1': [0.6024454582410083, 0.6108710746027519, 0.6047041150013008, 0.6191119677450525, 0.6119191773779407], 'system_2': [0.606662480082506, 0.6074457060513863, 0.6063754756051762, 0.6153840379016295, 0.6135077205968225], 'system_31': [0.5928405963883744, 0.6055086798912638, 0.59631347207985, 0.6160245821085454, 0.6087574648694185], 'system_32': [0.6027785556583438, 0.6094038484496643, 0.6039762734267515, 0.6168076212392345, 0.6091159817600809], 'system_33': [0.6031194130118972, 0.6089450994343056, 0.6040832911668199, 0.6154257640171429, 0.6142440399803106], 'system_34': [0.5971338541116138, 0.6113029673458217, 0.602344504324706, 0.6147625933078319, 0.6110243978694193], 'system_35': [0.6007992239656116, 0.6109143899290562, 0.6027626140965455, 0.614799522170766, 0.6090488694789006], 'system_4': [0.43261669932435926, 0.45019409174038094, 0.44610322162630645, 0.4568245086337954, 0.4404647909888684], 'system_5': [0.43122014091747773, 0.4446463766892016, 0.43747136580997636, 0.44856469635108676], 'system_6': [0.6144021854517727, 0.6235027887682232, 0.618802895551244, 0.6286475518027576], 'system_7': [0.6012381129363645, 0.6094900050870846, 0.6037404125668743, 0.6182132120619812]}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feff4a547d8c443a914e1a64fde920c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6781 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26d06a13eb349daa2bd783f14ca039e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/753 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "modes = {t: {name: [] for name, _ in operating_modes} for t in ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']}\n",
    "\n",
    "for train_idxs, val_idxs in indexes:\n",
    "    fold_dataset = DatasetDict(\n",
    "        {\"train\": ds.select(train_idxs), \"val\": ds.select(val_idxs)}\n",
    "    )\n",
    "\n",
    "    for name, preprocess_func in operating_modes:\n",
    "        curr_ds = fold_dataset.map(preprocess_func, batched=True).remove_columns(\n",
    "            fold_dataset[\"train\"].column_names\n",
    "        )\n",
    "\n",
    "        if name == \"system_4\":\n",
    "            ds_41 = fold_dataset.map(\n",
    "                preprocess_dataset_s41, batched=True\n",
    "            ).remove_columns(fold_dataset[\"train\"].column_names)\n",
    "            train_model(\n",
    "                \"system_41\",\n",
    "                AutoModelForSeq2SeqLM.from_pretrained(base_checkpoint),\n",
    "                ds_41,\n",
    "            )\n",
    "            ds_42 = fold_dataset.map(\n",
    "                preprocess_dataset_s42, batched=True\n",
    "            ).remove_columns(fold_dataset[\"train\"].column_names)\n",
    "            train_model(\n",
    "                \"system_42\",\n",
    "                AutoModelForSeq2SeqLM.from_pretrained(base_checkpoint),\n",
    "                ds_42,\n",
    "            )\n",
    "        elif name == \"system_5\":\n",
    "            pass\n",
    "        else:\n",
    "            if name == \"system_6\":\n",
    "                model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    \"RicoBorra/T5-small-synthetic-FLUTE\"\n",
    "                )\n",
    "            else:\n",
    "                model = AutoModelForSeq2SeqLM.from_pretrained(base_checkpoint)\n",
    "            trainer = train_model(name, model, curr_ds)\n",
    "\n",
    "        # have to do batched rouge computation otherwise not enough memory\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        metrics = {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0}\n",
    "        count = 0\n",
    "        for i in range(0, len(curr_ds[\"val\"]), N_GEN):\n",
    "            count += 1\n",
    "            if (name != \"system_4\") and (name != \"system_5\"):\n",
    "                (predictions, _), label_ids, _ = trainer.predict(\n",
    "                    test_dataset=curr_ds[\"val\"].select(\n",
    "                        range(i, min(i + N_GEN, len(curr_ds[\"val\"])))\n",
    "                    )\n",
    "                )\n",
    "                # delete stuff after EOS token\n",
    "                predicted_token_ids = torch.argmax(\n",
    "                    torch.from_numpy(predictions), dim=-1\n",
    "                )\n",
    "                for j in range(predicted_token_ids.shape[0]):\n",
    "                    ind = (predicted_token_ids[j] == 1).nonzero(as_tuple=True)[0]\n",
    "                    if ind.numel() != 0:\n",
    "                        predicted_token_ids[j, ind[0] :] = 1\n",
    "                decoded_preds = tokenizer.batch_decode(\n",
    "                    predicted_token_ids, skip_special_tokens=True\n",
    "                )\n",
    "                # clean decoded preds if needed\n",
    "                if name == \"system_2\":\n",
    "                    decoded_preds = [dp[dp.find(\"Label\") :] for dp in decoded_preds]\n",
    "                if name == \"system_7\":\n",
    "                    decoded_preds = [\n",
    "                        ex[ex.find(\"Label: \") :] + \" \" + ex[: ex.find(\"Label: \")]\n",
    "                        for ex in decoded_preds\n",
    "                    ]\n",
    "\n",
    "            else:\n",
    "                small_ds = curr_ds[\"val\"].select(\n",
    "                    range(i, min(i + N_GEN, len(curr_ds[\"val\"])))\n",
    "                )\n",
    "                label_ids = small_ds[\"labels\"]\n",
    "                max_len = max([len(el) for el in label_ids])\n",
    "                label_ids = [el + (max_len - len(el)) * [0] for el in label_ids]\n",
    "                inputs = tokenizer.batch_decode(\n",
    "                    small_ds[\"input_ids\"], skip_special_tokens=True\n",
    "                )\n",
    "                if name == \"system_4\":\n",
    "                    sys_4 = DREAM_FLUTE_System4(\n",
    "                        tokenizer=None,\n",
    "                        model_s41_path=get_path(\"system_41\"),\n",
    "                        model_s42_path=get_path(\"system_42\"),\n",
    "                    )\n",
    "                    decoded_preds = sys_4.prediction_pipeline(inputs=inputs)\n",
    "                else:  # \"name == system_5\"\n",
    "                    inputs = [get_prem_hyp(ex) for ex in inputs]\n",
    "                    sys_5 = DREAM_FLUTE_Ensemble(\n",
    "                        tokenizer_path=None,\n",
    "                        s1_path=get_path(\"system_1\"),\n",
    "                        s2_path=get_path(\"system_2\"),\n",
    "                        s3_emo_path=get_path(\"system_31\"),\n",
    "                        s3_mot_path=get_path(\"system_32\"),\n",
    "                        s3_cons_path=get_path(\"system_33\"),\n",
    "                        s3_rot_path=get_path(\"system_34\"),\n",
    "                        s3_alldims_path=get_path(\"system_35\"),\n",
    "                        s4_clas_path=get_path(\"system_41\"),\n",
    "                        s4_exp_path=get_path(\"system_42\"),\n",
    "                        dream_path=None,\n",
    "                    )\n",
    "                    decoded_preds = sys_5.predict(inputs=inputs)\n",
    "\n",
    "            # careful here\n",
    "            labels = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            if name == \"system_2\":\n",
    "                decoded_labels = [dp[dp.find(\"Label\") :] for dp in decoded_labels]\n",
    "            if name == \"system_7\":\n",
    "                decoded_labels = [\n",
    "                    ex[ex.find(\"Label: \") :] + \" \" + ex[: ex.find(\"Label: \")]\n",
    "                    for ex in decoded_labels\n",
    "                ]\n",
    "\n",
    "            new_metrics = rouge.compute(\n",
    "                predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "            )\n",
    "            for k in new_metrics:\n",
    "                metrics[k] += new_metrics[k]\n",
    "\n",
    "        for k in metrics:\n",
    "            metrics[k] /= count\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        for k in metrics:\n",
    "            modes[k][name].append(metrics[k])\n",
    "        print(modes)\n",
    "        # print(name, decoded_preds[0], decoded_labels[0])\n",
    "        records.append({\"name\": name, \"content\": decoded_preds[0]})\n",
    "        # print(predicted_token_ids[0], labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'system_33',\n",
       "  'content': 'Label: Entailment. Explanation: tintt glass is  transparent, it is notd and the fact of'},\n",
       " {'name': 'system_34',\n",
       "  'content': 'Label: Entailment. Explanation: tintt glass is  transparent, it is notd and the operation,'},\n",
       " {'name': 'system_35',\n",
       "  'content': 'Label: Contratailment. Explanation: tintted glass is  transparent, it is notd and the fact.'},\n",
       " {'name': 'system_4',\n",
       "  'content': 'Label: Entailment. Explanation: tinted glass is a transparent glass, so the simile is saying that the operation is obscured and not transparent.'},\n",
       " {'name': 'system_5',\n",
       "  'content': 'Label: Contradiction. Explanation: tinted glass is not transparent, so saying something is as transparent as tinted glass would mean it is not transparent.'},\n",
       " {'name': 'system_6',\n",
       "  'content': 'Label: Entailment. Explanation: tintted glass is not transparent, it is notd and the sunlight of'},\n",
       " {'name': 'system_7',\n",
       "  'content': 'Label: Entailment. Explanation: Tint glass is  transparent, it is d and the fact,'},\n",
       " {'name': 'ground_truth',\n",
       "  'content': 'Label: Entailment. Explanation: Tinted glass is not transparent because it is obscured by the color.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.append({\"name\": \"ground_truth\", \"content\": decoded_labels[0]})\n",
    "records[-12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n{'system_1': [0.5056115530372459, 0.5085812472965738], 'system_2': [0.5323239453833644, 0.5374525863206762], 'system_31': [0.5084813364883226, 0.5078069259932709], 'system_32': [0.5055255195638098, 0.5074808302929493], 'system_33': [0.5032615710245999, 0.508591734512484], 'system_34': [0.5067396302748032, 0.5087587260751042], 'system_35': [0.5036829298423671, 0.5073656197676405]}\\n{'system_1': [0.47329270980754923], 'system_2': [0.5039333868999851], 'system_31': [0.47420432731446144], 'system_32': [0.473849787491779], 'system_33': [0.47287909150934293], 'system_34': [0.47171914165316353], 'system_35': [0.47436950304876985]}\\n{'system_1': [0.5221458462682274, 0.4906728974226775], 'system_2': [0.5453307495230577, 0.5189864534869755], 'system_31': [0.524029905213074, 0.49432447432649923], 'system_32': [0.5236538569098267, 0.4949640794043393], 'system_33': [0.522696825432352, 0.49418902866839187], 'system_34': [0.5231002650281118, 0.49245360907178504], 'system_35': [0.5238553839735753, 0.4967562420314123]}\\n{'system_1': [0.5201383622010409, 0.5019083214066056], 'system_2': [0.5451744732719431, 0.5273190341222926], 'system_31': [0.5192974336052382, 0.5002995114825706], 'system_32': [0.5188841013023715, 0.5007719063422055], 'system_33': [0.5203219622545853, 0.49992736417041994], 'system_34': [0.5180675680247785, 0.5008015168679895], 'system_35': [0.5224425242712794, 0.5010754435268072]}\\n{'system_1': [0.49590791112913984], 'system_2': [0.5147096144113823], 'system_31': [0.4927430972029428], 'system_32': [0.49258799343269694], 'system_33': [0.49049952341283654], 'system_34': [0.49317378483047924], 'system_35': [0.4919880906784929]}\\n{'system_1': [0.498351050356349, 0.5090844074007058], 'system_2': [0.5289706096196247, 0.535423121872322], 'system_31': [0.5004418699772601, 0.5081924349139884], 'system_32': [0.5000023839289528, 0.5069172466751917], 'system_33': [0.49609349538092806, 0.5060478473679386], 'system_34': [0.49683869684920356, 0.5072891134960673], 'system_35': [0.49873229852851614, 0.5063988604083957]}\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "{'system_1': [0.5056115530372459, 0.5085812472965738], 'system_2': [0.5323239453833644, 0.5374525863206762], 'system_31': [0.5084813364883226, 0.5078069259932709], 'system_32': [0.5055255195638098, 0.5074808302929493], 'system_33': [0.5032615710245999, 0.508591734512484], 'system_34': [0.5067396302748032, 0.5087587260751042], 'system_35': [0.5036829298423671, 0.5073656197676405]}\n",
    "{'system_1': [0.47329270980754923], 'system_2': [0.5039333868999851], 'system_31': [0.47420432731446144], 'system_32': [0.473849787491779], 'system_33': [0.47287909150934293], 'system_34': [0.47171914165316353], 'system_35': [0.47436950304876985]}\n",
    "{'system_1': [0.5221458462682274, 0.4906728974226775], 'system_2': [0.5453307495230577, 0.5189864534869755], 'system_31': [0.524029905213074, 0.49432447432649923], 'system_32': [0.5236538569098267, 0.4949640794043393], 'system_33': [0.522696825432352, 0.49418902866839187], 'system_34': [0.5231002650281118, 0.49245360907178504], 'system_35': [0.5238553839735753, 0.4967562420314123]}\n",
    "{'system_1': [0.5201383622010409, 0.5019083214066056], 'system_2': [0.5451744732719431, 0.5273190341222926], 'system_31': [0.5192974336052382, 0.5002995114825706], 'system_32': [0.5188841013023715, 0.5007719063422055], 'system_33': [0.5203219622545853, 0.49992736417041994], 'system_34': [0.5180675680247785, 0.5008015168679895], 'system_35': [0.5224425242712794, 0.5010754435268072]}\n",
    "{'system_1': [0.49590791112913984], 'system_2': [0.5147096144113823], 'system_31': [0.4927430972029428], 'system_32': [0.49258799343269694], 'system_33': [0.49049952341283654], 'system_34': [0.49317378483047924], 'system_35': [0.4919880906784929]}\n",
    "{'system_1': [0.498351050356349, 0.5090844074007058], 'system_2': [0.5289706096196247, 0.535423121872322], 'system_31': [0.5004418699772601, 0.5081924349139884], 'system_32': [0.5000023839289528, 0.5069172466751917], 'system_33': [0.49609349538092806, 0.5060478473679386], 'system_34': [0.49683869684920356, 0.5072891134960673], 'system_35': [0.49873229852851614, 0.5063988604083957]}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': {'system_1': [0.6466564938196772,\n",
       "   0.6374162664520232,\n",
       "   0.6314581094782346],\n",
       "  'system_2': [0.647300237510074, 0.6398066286487966, 0.6316403011934265],\n",
       "  'system_31': [0.6436652005295087, 0.6381424096015224, 0.6329557740838078],\n",
       "  'system_32': [0.6435581473896589, 0.6351847311087284, 0.6289004178174992],\n",
       "  'system_33': [0.6483284197355579, 0.6384891439272665, 0.6301390557884653],\n",
       "  'system_34': [0.6456405508577208, 0.640969872673464, 0.6304813299839308],\n",
       "  'system_35': [0.6427587187656778, 0.6398745098953657, 0.6257141235790099],\n",
       "  'system_4': [0.5149894127994684, 0.5064390404381742, 0.5012878790775165],\n",
       "  'system_5': [0.5120373781541929, 0.48918339633235103, 0.4901123976312988],\n",
       "  'system_6': [0.6483702759709798, 0.6490100518589779, 0.6401491143045684],\n",
       "  'system_7': [0.6494594041083502, 0.6451021144716681, 0.638140882622206]},\n",
       " 'rouge2': {'system_1': [0.3874339057864731,\n",
       "   0.37345106765912295,\n",
       "   0.3714762367806833],\n",
       "  'system_2': [0.3907885498263465, 0.37744053481711626, 0.36932541325543644],\n",
       "  'system_31': [0.38630398897356893, 0.3726942565905078, 0.37127193470339265],\n",
       "  'system_32': [0.38543534193629136, 0.36787807403280876, 0.3641835960897663],\n",
       "  'system_33': [0.39189171430663994, 0.37177698243979207, 0.3658444329987311],\n",
       "  'system_34': [0.38493414949981725, 0.3733642234060303, 0.3641080508136351],\n",
       "  'system_35': [0.3865191929458037, 0.3720793061552449, 0.36345656274734484],\n",
       "  'system_4': [0.2877697286265143, 0.2754848928997868, 0.27437594452852154],\n",
       "  'system_5': [0.28066694124298563, 0.25584751064391964, 0.2624561087714946],\n",
       "  'system_6': [0.3931806808845674, 0.3946715951114686, 0.38824458254661914],\n",
       "  'system_7': [0.39013176464039545, 0.3825197433166929, 0.3782520851895826]},\n",
       " 'rougeL': {'system_1': [0.6117986691783841,\n",
       "   0.6018339906349851,\n",
       "   0.599323402456295],\n",
       "  'system_2': [0.6135966818704379, 0.6044971223489091, 0.5998073370796452],\n",
       "  'system_31': [0.6088470404992611, 0.6020395493937137, 0.601205313579512],\n",
       "  'system_32': [0.6091921447983717, 0.6020107685419468, 0.5976175755555985],\n",
       "  'system_33': [0.614075997353156, 0.603203601377491, 0.5976923265564966],\n",
       "  'system_34': [0.6109343715888967, 0.6061403681268476, 0.5962433257260182],\n",
       "  'system_35': [0.6090485560712955, 0.6034312573786255, 0.5921302387568552],\n",
       "  'system_4': [0.4402633432754091, 0.4362376808895232, 0.4316091725862533],\n",
       "  'system_5': [0.4395399853079367, 0.41925665293028147, 0.4199399193275206],\n",
       "  'system_6': [0.6168191779398146, 0.6173846979526544, 0.6099367500298917],\n",
       "  'system_7': [0.6078649553728641, 0.6031424512835705, 0.6022406070480769]},\n",
       " 'rougeLsum': {'system_1': [0.6119191773779407,\n",
       "   0.6018128912371763,\n",
       "   0.5995153448014845],\n",
       "  'system_2': [0.6135077205968225, 0.6045350943570997, 0.5998438558597035],\n",
       "  'system_31': [0.6087574648694185, 0.6016661600232002, 0.601502632208731],\n",
       "  'system_32': [0.6091159817600809, 0.6018432084731631, 0.5977236905330123],\n",
       "  'system_33': [0.6142440399803106, 0.6030525824568761, 0.5978672726185856],\n",
       "  'system_34': [0.6110243978694193, 0.6062049581667269, 0.5962521552130583],\n",
       "  'system_35': [0.6090488694789006, 0.6031398799512177, 0.5922765673040742],\n",
       "  'system_4': [0.4404647909888684, 0.43620743281404784, 0.4316292446922213],\n",
       "  'system_5': [0.43979205406206534, 0.4191914319174704, 0.42018118584657793],\n",
       "  'system_6': [0.6167806953841022, 0.6174896577963155, 0.6098576353382814],\n",
       "  'system_7': [0.6078548534818653, 0.6028331791158014, 0.6025368982784524]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1\n",
      "--- system_2 TtestResult(statistic=1.5942866286854278, pvalue=0.1259541828244727, df=2)\n",
      "--- system_31 TtestResult(statistic=-0.1846141109413712, pvalue=0.5647218063475055, df=2)\n",
      "--- system_32 TtestResult(statistic=-10.401611334061817, pvalue=0.995441750397509, df=2)\n",
      "--- system_33 TtestResult(statistic=0.5201569284801854, pvalue=0.3274012716270326, df=2)\n",
      "--- system_34 TtestResult(statistic=0.3430443005297469, pvalue=0.3821335760393537, df=2)\n",
      "--- system_35 TtestResult(statistic=-0.9638874716253307, pvalue=0.7815987955454509, df=2)\n",
      "--- system_4 TtestResult(statistic=-302.71616824427184, pvalue=0.9999945437826719, df=2)\n",
      "--- system_5 TtestResult(statistic=-35.979050166289326, pvalue=0.9996141951070484, df=2)\n",
      "--- system_6 TtestResult(statistic=2.501111940461156, pvalue=0.06475895921453635, df=2)\n",
      "--- system_7 TtestResult(statistic=3.844355677405259, pvalue=0.03074423644019506, df=2)\n",
      "rouge2\n",
      "--- system_2 TtestResult(statistic=0.8879273824343987, pvalue=0.23413036119116537, df=2)\n",
      "--- system_31 TtestResult(statistic=-2.5923706610031836, pvalue=0.9389340339926685, df=2)\n",
      "--- system_32 TtestResult(statistic=-3.177700379129172, pvalue=0.9568042273045234, df=2)\n",
      "--- system_33 TtestResult(statistic=-0.32345350163513914, pvalue=0.6114794432729286, df=2)\n",
      "--- system_34 TtestResult(statistic=-1.549566690183321, pvalue=0.8693149168808644, df=2)\n",
      "--- system_35 TtestResult(statistic=-1.4962861254733804, pvalue=0.8633787221148874, df=2)\n",
      "--- system_4 TtestResult(statistic=-130.46754760558434, pvalue=0.999970628458693, df=2)\n",
      "--- system_5 TtestResult(statistic=-33.66176728647794, pvalue=0.99955932163455, df=2)\n",
      "--- system_6 TtestResult(statistic=3.169857668441417, pvalue=0.04338273373227104, df=2)\n",
      "--- system_7 TtestResult(statistic=3.317642816346917, pvalue=0.04004518375413225, df=2)\n",
      "rougeL\n",
      "--- system_2 TtestResult(statistic=2.601928729594382, pvalue=0.060696377765290556, df=2)\n",
      "--- system_31 TtestResult(statistic=-0.20328619600772474, pvalue=0.5711412983348967, df=2)\n",
      "--- system_32 TtestResult(statistic=-1.681194866895825, pvalue=0.882627055369884, df=2)\n",
      "--- system_33 TtestResult(statistic=0.5689933646868743, pvalue=0.31336968780758157, df=2)\n",
      "--- system_34 TtestResult(statistic=0.05513920255913593, pvalue=0.4805201486600571, df=2)\n",
      "--- system_35 TtestResult(statistic=-1.0963003605380535, pvalue=0.8063355574951545, df=2)\n",
      "--- system_4 TtestResult(statistic=-96.83692153115922, pvalue=0.9999466887835425, df=2)\n",
      "--- system_5 TtestResult(statistic=-58.38585031383625, pvalue=0.9998533899517065, df=2)\n",
      "--- system_6 TtestResult(statistic=3.4173715243185887, pvalue=0.03799773564831821, df=2)\n",
      "--- system_7 TtestResult(statistic=0.0470511547111988, pvalue=0.4833741038242195, df=2)\n",
      "rougeLsum\n",
      "--- system_2 TtestResult(statistic=2.2369068920316804, pvalue=0.07737759371107805, df=2)\n",
      "--- system_31 TtestResult(statistic=-0.2948438675273773, pvalue=0.6020487901252163, df=2)\n",
      "--- system_32 TtestResult(statistic=-1.8352639592018827, pvalue=0.8960540396238271, df=2)\n",
      "--- system_33 TtestResult(statistic=0.5388289785950215, pvalue=0.321978979787935, df=2)\n",
      "--- system_34 TtestResult(statistic=0.03448529444753662, pvalue=0.4878112305133725, df=2)\n",
      "--- system_35 TtestResult(statistic=-1.1837839909647274, pvalue=0.8209350529441062, df=2)\n",
      "--- system_4 TtestResult(statistic=-98.89126121843584, pvalue=0.999948880385983, df=2)\n",
      "--- system_5 TtestResult(statistic=-57.44505893948846, pvalue=0.999848550693174, df=2)\n",
      "--- system_6 TtestResult(statistic=3.2968941165054044, pvalue=0.04049103989543848, df=2)\n",
      "--- system_7 TtestResult(statistic=-0.0035532919685574062, pvalue=0.5012562744578637, df=2)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# paired t test\n",
    "for mode in modes:\n",
    "    print(mode)\n",
    "    for key in list(modes[mode].keys())[1:]:\n",
    "        print(\"---\", key, ttest_rel(modes[mode][key], modes[mode][\"system_1\"], alternative=\"greater\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
