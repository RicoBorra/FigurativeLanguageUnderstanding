{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 12\n",
    "NUM_EPOCHS = 8\n",
    "N_GEN = 50\n",
    "base_checkpoint = \"t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_checkpoint)\n",
    "\n",
    "# ds = load_dataset(\"ColumbiaNLP/FLUTE\").shuffle(seed=42)\n",
    "df = pd.read_csv(\"complete_dataset.csv\").fillna(\"\")\n",
    "ds = Dataset.from_pandas(df).shuffle(seed=42)\n",
    "folds = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "splits = folds.split(ds, ds[\"label\"])\n",
    "indexes = [t for t in splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ac44b6b7da4c989308e66c61f62be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7534 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from flute_dream import add_combined_cols\n",
    "\n",
    "ds = ds.map(add_combined_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_s1(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis\"])\n",
    "    labels = tokenizer(examples[\"label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s2(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_system_2\"])\n",
    "    labels = tokenizer(examples[\"type_label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s31(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_emotion\"])\n",
    "    labels = tokenizer(examples[\"label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s32(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_motivation\"])\n",
    "    labels = tokenizer(examples[\"label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s33(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_consequence\"])\n",
    "    labels = tokenizer(examples[\"label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s34(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_rot\"])\n",
    "    labels = tokenizer(examples[\"label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s35(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_all_dims\"])\n",
    "    labels = tokenizer(examples[\"label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s41(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis\"])\n",
    "    labels = tokenizer(examples[\"label\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s42(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_label\"])\n",
    "    labels = tokenizer(examples[\"explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s7(examples):\n",
    "    lbl_exp = [\n",
    "        ex[ex.find(\"Explanation: \") :] + ex[: ex.find(\"Explanation: \")]\n",
    "        for ex in examples[\"label_explanation\"]\n",
    "    ]\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis\"])\n",
    "    labels = tokenizer(lbl_exp)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def get_path(name):\n",
    "    return f\"{name}/{os.listdir(name)[0]}\"\n",
    "\n",
    "\n",
    "def get_prem_hyp(s):\n",
    "    ind = s.find(\"Hypothesis: \")\n",
    "    prem = s[len(\"Premise: \") : ind]\n",
    "    hyp = s[ind + len(\"Hypothesis: \") :]\n",
    "    return [prem, hyp]\n",
    "\n",
    "\n",
    "\"\"\"Class encapsulating the two steps of System 4 (Classify, then Explain)\"\"\"\n",
    "\n",
    "\n",
    "class DREAM_FLUTE_System4:\n",
    "    def __init__(\n",
    "        self, tokenizer=None, model_s41_path=None, model_s42_path=None\n",
    "    ) -> None:\n",
    "        self.tokenizer = (\n",
    "            tokenizer\n",
    "            if tokenizer is not None\n",
    "            else AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "        )\n",
    "        self.model_s41 = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(model_s41_path)\n",
    "            if model_s41_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                \"YoanBOUTE/DREAM-FLUTE-S4-Classify\"\n",
    "            )\n",
    "        )\n",
    "        self.model_s42 = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(model_s42_path)\n",
    "            if model_s42_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                \"YoanBOUTE/DREAM-FLUTE-S4-Explain\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    \"\"\"Expected input for function : \"Premise: ... . Hypothesis: ... . Is there a contradiction or entailment between the premise and hypothesis ?\" \n",
    "    Or list of strings in this format\"\"\"\n",
    "\n",
    "    def prediction_pipeline(self, inputs):\n",
    "        if isinstance(inputs, str):\n",
    "            tok_input = self.tokenizer(inputs, return_tensors=\"pt\").input_ids\n",
    "            output_model_1 = self.model_s41.generate(tok_input, max_new_tokens=100)\n",
    "            decoded_output_model_1 = \"Label: \" + self.tokenizer.decode(\n",
    "                output_model_1[0], skip_special_tokens=True\n",
    "            )\n",
    "            intermediate_input = (\n",
    "                inputs[\n",
    "                    : inputs.find(\n",
    "                        \"Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "                    )\n",
    "                ]\n",
    "                + decoded_output_model_1\n",
    "                + \". What is the explanation of the label associated to the premise and the hypothesis ?\"\n",
    "            )\n",
    "            tok_intermediate_input = self.tokenizer(\n",
    "                intermediate_input, return_tensors=\"pt\"\n",
    "            ).input_ids\n",
    "            output_model_2 = self.model_s42.generate(\n",
    "                tok_intermediate_input, max_new_tokens=100\n",
    "            )\n",
    "\n",
    "            return (\n",
    "                decoded_output_model_1\n",
    "                + \". Explanation: \"\n",
    "                + self.tokenizer.decode(output_model_2[0], skip_special_tokens=True)\n",
    "            )\n",
    "\n",
    "        elif isinstance(inputs, list) and all(\n",
    "            isinstance(input, str) for input in inputs\n",
    "        ):\n",
    "            predictions = []\n",
    "            for input in inputs:\n",
    "                predictions.append(self.prediction_pipeline(input))\n",
    "\n",
    "            return predictions\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"Inputs should be either a list of two strings or a list of lists of two strings\"\n",
    "            )\n",
    "\n",
    "\n",
    "\"\"\"Ensemble class that loads all models from HuggingFace (or from the device if a path to the model is indicated) \n",
    "and implements the ensembling algorithm given in the DREAM-FLUTE paper\"\"\"\n",
    "\n",
    "\n",
    "class DREAM_FLUTE_Ensemble:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer_path=None,\n",
    "        s1_path=None,\n",
    "        s2_path=None,\n",
    "        s3_emo_path=None,\n",
    "        s3_mot_path=None,\n",
    "        s3_cons_path=None,\n",
    "        s3_rot_path=None,\n",
    "        s3_alldims_path=None,\n",
    "        s4_clas_path=None,\n",
    "        s4_exp_path=None,\n",
    "        dream_path=None,\n",
    "    ) -> None:\n",
    "        self.tokenizer = (\n",
    "            AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "            if tokenizer_path is not None\n",
    "            else AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "        )\n",
    "        self.model_s1 = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s1_path)\n",
    "            if s1_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\"YoanBOUTE/DREAM-FLUTE-S1\")\n",
    "        )\n",
    "        self.model_s2 = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s2_path)\n",
    "            if s2_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\"YoanBOUTE/DREAM-FLUTE-S2\")\n",
    "        )\n",
    "        self.model_s3_emo = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s3_emo_path)\n",
    "            if s3_emo_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                \"YoanBOUTE/DREAM-FLUTE-S3-Emotion\"\n",
    "            )\n",
    "        )\n",
    "        self.model_s3_mot = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s3_mot_path)\n",
    "            if s3_mot_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                \"YoanBOUTE/DREAM-FLUTE-S3-Motivation\"\n",
    "            )\n",
    "        )\n",
    "        self.model_s3_cons = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s3_cons_path)\n",
    "            if s3_cons_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                \"YoanBOUTE/DREAM-FLUTE-S3-Consequence\"\n",
    "            )\n",
    "        )\n",
    "        self.model_s3_rot = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s3_rot_path)\n",
    "            if s3_rot_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\"YoanBOUTE/DREAM-FLUTE-S3-ROT\")\n",
    "        )\n",
    "        self.model_s3_alldims = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s3_alldims_path)\n",
    "            if s3_alldims_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                \"YoanBOUTE/DREAM-FLUTE-S3-AllDims\"\n",
    "            )\n",
    "        )\n",
    "        self.model_s4 = DREAM_FLUTE_System4(self.tokenizer, s4_clas_path, s4_exp_path)\n",
    "        self.model_dream = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(dream_path)\n",
    "            if dream_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\"RicoBorra/DREAM-t5-small\")\n",
    "        )\n",
    "\n",
    "    \"\"\"Tokenizes the input, then feeds it to the given model, and decodes the output to have a string as result.\n",
    "    This method is callable for all models except System 4 (Use the method defined in the class of System 4)\"\"\"\n",
    "\n",
    "    def _prediction_pipeline(self, input: str, model) -> str:\n",
    "        tokenized_input = self.tokenizer(input, return_tensors=\"pt\").input_ids\n",
    "        model_output = model.generate(tokenized_input, max_new_tokens=100)\n",
    "        decoded_output = self.tokenizer.decode(\n",
    "            model_output[0], skip_special_tokens=True\n",
    "        )\n",
    "        return decoded_output\n",
    "\n",
    "    \"\"\"Preprocesses the input for each model, then feeds it to the pipeline.\n",
    "    Returns a dictionary of all models' predictions.\"\"\"\n",
    "\n",
    "    def _get_all_predictions(self, input: list):\n",
    "        prem, hyp = input\n",
    "        prem = prem.strip()\n",
    "        hyp = hyp.strip()\n",
    "        if not prem.endswith(\".\"):\n",
    "            prem += \".\"\n",
    "        if not hyp.endswith(\".\"):\n",
    "            hyp += \".\"\n",
    "\n",
    "        predictions = dict()\n",
    "\n",
    "        input_1 = f\"Premise: {prem} Hypothesis: {hyp} Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S1\"] = self._prediction_pipeline(input_1, self.model_s1)\n",
    "\n",
    "        input_2 = f\"Premise: {prem} Hypothesis: {hyp} What is the type of figurative language involved? Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S2\"] = self._prediction_pipeline(input_2, self.model_s2)\n",
    "\n",
    "        # DREAM elaborations for system 3\n",
    "        input_dream_prem = f\"[SITUATION] {prem} [QUERY] \"\n",
    "        input_dream_hyp = f\"[SITUATION] {hyp} [QUERY] \"\n",
    "        prem_elaborations = {\n",
    "            key: self._prediction_pipeline(input_dream_prem + key, self.model_dream)\n",
    "            for key in [\"emotion\", \"motivation\", \"consequence\", \"rot\"]\n",
    "        }\n",
    "        for key, elab in prem_elaborations.items():\n",
    "            elab = elab.strip()\n",
    "            if not elab.endswith(\".\"):\n",
    "                prem_elaborations[key] += \".\"\n",
    "        hyp_elaborations = {\n",
    "            key: self._prediction_pipeline(input_dream_hyp + key, self.model_dream)\n",
    "            for key in [\"emotion\", \"motivation\", \"consequence\", \"rot\"]\n",
    "        }\n",
    "        for key, elab in hyp_elaborations.items():\n",
    "            elab = elab.strip()\n",
    "            if not elab.endswith(\".\"):\n",
    "                hyp_elaborations[key] += \".\"\n",
    "\n",
    "        input_3_emo = f\"Premise: {prem} [Emotion] {prem_elaborations['emotion']} Hypothesis: {hyp} [Emotion] {hyp_elaborations['emotion']} Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S3-emo\"] = self._prediction_pipeline(\n",
    "            input_3_emo, self.model_s3_emo\n",
    "        )\n",
    "\n",
    "        input_3_mot = f\"Premise: {prem} [Motivation] {prem_elaborations['motivation']} Hypothesis: {hyp} [Motivation] {hyp_elaborations['motivation']} Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S3-mot\"] = self._prediction_pipeline(\n",
    "            input_3_mot, self.model_s3_mot\n",
    "        )\n",
    "\n",
    "        input_3_cons = f\"Premise: {prem} [Consequence] {prem_elaborations['consequence']} Hypothesis: {hyp} [Consequence] {hyp_elaborations['consequence']} Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S3-cons\"] = self._prediction_pipeline(\n",
    "            input_3_cons, self.model_s3_cons\n",
    "        )\n",
    "\n",
    "        input_3_rot = f\"Premise: {prem} [Rot] {prem_elaborations['rot']} Hypothesis: {hyp} [Rot] {hyp_elaborations['rot']} Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S3-rot\"] = self._prediction_pipeline(\n",
    "            input_3_rot, self.model_s3_rot\n",
    "        )\n",
    "\n",
    "        input_3_all = f\"Premise: {prem} \"\n",
    "        for key, elab in prem_elaborations.items():\n",
    "            input_3_all += f\"[{key.capitalize()}] {elab} \"\n",
    "        input_3_all += f\"Hypothesis: {hyp} \"\n",
    "        for key, elab in hyp_elaborations.items():\n",
    "            input_3_all += f\"[{key.capitalize()}] {elab} \"\n",
    "        input_3_all += \"Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S3-all\"] = self._prediction_pipeline(\n",
    "            input_3_all, self.model_s3_alldims\n",
    "        )\n",
    "\n",
    "        # The input for system 4 is in the same format as for system 1\n",
    "        predictions[\"S4\"] = self.model_s4.prediction_pipeline(input_1)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    \"\"\"Uses the predictions from each model to compute the final prediction of the ensemble\"\"\"\n",
    "\n",
    "    def _ensemble_algorithm(self, model_outputs):\n",
    "        # Firstly, the label is selected based on the majority between the 5 best models (according to the paper : systems 1, 2, 3-motivation, 3-alldims, 4)\n",
    "        labels = [\n",
    "            model_outputs[key].split(\".\")[0]\n",
    "            for key in [\"S1\", \"S2\", \"S3-mot\", \"S3-all\", \"S4\"]\n",
    "        ]\n",
    "        # Sometimes, it might happen with the small models that the generated label is a mix of words, like 'Contratailment' or 'Endiction'\n",
    "        for label in labels:\n",
    "            if label not in [\"Label: Contradiction\", \"Label: Entailment\"]:\n",
    "                labels.remove(label)\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        ix = np.argmax(counts)\n",
    "        major_label = unique[ix]\n",
    "\n",
    "        # Then, pick the explanation of the first system agreeing with the majority label, following an order indicated in the paper\n",
    "        for key in [\"S3-cons\", \"S3-emo\", \"S2\", \"S3-all\", \"S3-mot\", \"S4\", \"S1\"]:\n",
    "            substrings = model_outputs[key].split(\".\")\n",
    "            label = substrings[0]\n",
    "            explanation = substrings[1]\n",
    "\n",
    "            if label == major_label:\n",
    "                break\n",
    "\n",
    "        return major_label + \".\" + explanation + \".\"\n",
    "\n",
    "    \"\"\"Expected input : [Premise_sentence, hypothesis_sentence] or list of inputs\"\"\"\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        if (\n",
    "            isinstance(inputs, list)\n",
    "            and all(isinstance(input, str) for input in inputs)\n",
    "            and len(inputs) == 2\n",
    "        ):\n",
    "            preds = self._get_all_predictions(inputs)\n",
    "            final_pred = self._ensemble_algorithm(preds)\n",
    "\n",
    "            return final_pred\n",
    "\n",
    "        elif isinstance(inputs, list) and all(\n",
    "            isinstance(input, list) for input in inputs\n",
    "        ):\n",
    "            predictions = []\n",
    "            for input in inputs:\n",
    "                predictions.append(self.predict(input))\n",
    "\n",
    "            return predictions\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"Inputs should be either a list of two strings or a list of lists of two strings\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "operating_modes = [\n",
    "    (\"system_1\", preprocess_dataset_s1),\n",
    "    (\"system_2\", preprocess_dataset_s2),\n",
    "    (\"system_31\", preprocess_dataset_s31),\n",
    "    (\"system_32\", preprocess_dataset_s32),\n",
    "    (\"system_33\", preprocess_dataset_s33),\n",
    "    (\"system_34\", preprocess_dataset_s34),\n",
    "    (\"system_35\", preprocess_dataset_s35),\n",
    "    (\"system_4\", preprocess_dataset_s1),\n",
    "    (\"system_5\", preprocess_dataset_s1),\n",
    "    (\"system_6\", preprocess_dataset_s1),\n",
    "    (\"system_7\", preprocess_dataset_s7),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(name, model, curr_ds):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"{name}\",\n",
    "        learning_rate=3e-4,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=2 * BATCH_SIZE,\n",
    "        save_total_limit=1,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        report_to=\"none\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        eval_accumulation_steps=1,\n",
    "        logging_steps=1,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=curr_ds[\"train\"],\n",
    "        eval_dataset=curr_ds[\"val\"].select(range(350)),\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'system_1': [0.6544642734433673, 0.6538472479091643, 0.6451995135621926], 'system_2': [0.6591784410473625, 0.6556348348860563, 0.6485904916673478], 'system_31': [0.6616151178011304, 0.6529529681168658, 0.6457822724848103], 'system_32': [0.6602817794390899, 0.6454009215237158, 0.6454316035156613], 'system_33': [0.6577626817560993, 0.654545940696381, 0.6495673748102144], 'system_34': [0.6577570152042305, 0.6535457188788231, 0.6434608160222891], 'system_35': [0.6555744419703384, 0.6538046517382146, 0.6470891971959015], 'system_4': [0.52284036210826, 0.5163325460813661, 0.5156399803184772], 'system_5': [0.5240891672641392, 0.5080151274468501, 0.5057846437218605], 'system_6': [0.6635674632696552, 0.6627904866947751, 0.6585561479873518], 'system_7': [0.6611524853455227, 0.6590132359931218, 0.6561421023328687]}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "modes = {t: {name: [] for name, _ in operating_modes} for t in ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']}\n",
    "\n",
    "for train_idxs, val_idxs in indexes[4:7]:\n",
    "    fold_dataset = DatasetDict(\n",
    "        {\"train\": ds.select(train_idxs), \"val\": ds.select(val_idxs)}\n",
    "    )\n",
    "\n",
    "    for name, preprocess_func in operating_modes:\n",
    "        curr_ds = fold_dataset.map(preprocess_func, batched=True).remove_columns(\n",
    "            fold_dataset[\"train\"].column_names\n",
    "        )\n",
    "\n",
    "        if name == \"system_4\":\n",
    "            ds_41 = fold_dataset.map(\n",
    "                preprocess_dataset_s41, batched=True\n",
    "            ).remove_columns(fold_dataset[\"train\"].column_names)\n",
    "            train_model(\n",
    "                \"system_41\",\n",
    "                AutoModelForSeq2SeqLM.from_pretrained(base_checkpoint),\n",
    "                ds_41,\n",
    "            )\n",
    "            ds_42 = fold_dataset.map(\n",
    "                preprocess_dataset_s42, batched=True\n",
    "            ).remove_columns(fold_dataset[\"train\"].column_names)\n",
    "            train_model(\n",
    "                \"system_42\",\n",
    "                AutoModelForSeq2SeqLM.from_pretrained(base_checkpoint),\n",
    "                ds_42,\n",
    "            )\n",
    "        elif name == \"system_5\":\n",
    "            pass\n",
    "        else:\n",
    "            if name == \"system_6\":\n",
    "                model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    \"RicoBorra/T5-small-synthetic-FLUTE\"\n",
    "                )\n",
    "            else:\n",
    "                model = AutoModelForSeq2SeqLM.from_pretrained(base_checkpoint)\n",
    "            trainer = train_model(name, model, curr_ds)\n",
    "\n",
    "        # have to do batched rouge computation otherwise not enough memory\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        metrics = {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0}\n",
    "        count = 0\n",
    "        for i in range(0, len(curr_ds[\"val\"]), N_GEN):\n",
    "            count += 1\n",
    "            if (name != \"system_4\") and (name != \"system_5\"):\n",
    "                (predictions, _), label_ids, _ = trainer.predict(\n",
    "                    test_dataset=curr_ds[\"val\"].select(\n",
    "                        range(i, min(i + N_GEN, len(curr_ds[\"val\"])))\n",
    "                    )\n",
    "                )\n",
    "                # delete stuff after EOS token\n",
    "                predicted_token_ids = torch.argmax(\n",
    "                    torch.from_numpy(predictions), dim=-1\n",
    "                )\n",
    "                for j in range(predicted_token_ids.shape[0]):\n",
    "                    ind = (predicted_token_ids[j] == 1).nonzero(as_tuple=True)[0]\n",
    "                    if ind.numel() != 0:\n",
    "                        predicted_token_ids[j, ind[0] :] = 1\n",
    "                decoded_preds = tokenizer.batch_decode(\n",
    "                    predicted_token_ids, skip_special_tokens=True\n",
    "                )\n",
    "                # clean decoded preds if needed\n",
    "                if name == \"system_2\":\n",
    "                    decoded_preds = [dp[dp.find(\"Label\") :] for dp in decoded_preds]\n",
    "                if name == \"system_7\":\n",
    "                    decoded_preds = [\n",
    "                        ex[ex.find(\"Label: \") :] + \" \" + ex[: ex.find(\"Label: \")]\n",
    "                        for ex in decoded_preds\n",
    "                    ]\n",
    "\n",
    "            else:\n",
    "                small_ds = curr_ds[\"val\"].select(\n",
    "                    range(i, min(i + N_GEN, len(curr_ds[\"val\"])))\n",
    "                )\n",
    "                label_ids = small_ds[\"labels\"]\n",
    "                max_len = max([len(el) for el in label_ids])\n",
    "                label_ids = [el + (max_len - len(el)) * [0] for el in label_ids]\n",
    "                inputs = tokenizer.batch_decode(\n",
    "                    small_ds[\"input_ids\"], skip_special_tokens=True\n",
    "                )\n",
    "                if name == \"system_4\":\n",
    "                    sys_4 = DREAM_FLUTE_System4(\n",
    "                        tokenizer=None,\n",
    "                        model_s41_path=get_path(\"system_41\"),\n",
    "                        model_s42_path=get_path(\"system_42\"),\n",
    "                    )\n",
    "                    decoded_preds = sys_4.prediction_pipeline(inputs=inputs)\n",
    "                else:  # \"name == system_5\"\n",
    "                    inputs = [get_prem_hyp(ex) for ex in inputs]\n",
    "                    sys_5 = DREAM_FLUTE_Ensemble(\n",
    "                        tokenizer_path=None,\n",
    "                        s1_path=get_path(\"system_1\"),\n",
    "                        s2_path=get_path(\"system_2\"),\n",
    "                        s3_emo_path=get_path(\"system_31\"),\n",
    "                        s3_mot_path=get_path(\"system_32\"),\n",
    "                        s3_cons_path=get_path(\"system_33\"),\n",
    "                        s3_rot_path=get_path(\"system_34\"),\n",
    "                        s3_alldims_path=get_path(\"system_35\"),\n",
    "                        s4_clas_path=get_path(\"system_41\"),\n",
    "                        s4_exp_path=get_path(\"system_42\"),\n",
    "                        dream_path=None,\n",
    "                    )\n",
    "                    decoded_preds = sys_5.predict(inputs=inputs)\n",
    "\n",
    "            # careful here\n",
    "            labels = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            if name == \"system_2\":\n",
    "                decoded_labels = [dp[dp.find(\"Label\") :] for dp in decoded_labels]\n",
    "            if name == \"system_7\":\n",
    "                decoded_labels = [\n",
    "                    ex[ex.find(\"Label: \") :] + \" \" + ex[: ex.find(\"Label: \")]\n",
    "                    for ex in decoded_labels\n",
    "                ]\n",
    "\n",
    "            new_metrics = rouge.compute(\n",
    "                predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "            )\n",
    "            for k in new_metrics:\n",
    "                metrics[k] += new_metrics[k]\n",
    "\n",
    "        for k in metrics:\n",
    "            metrics[k] /= count\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        for k in metrics:\n",
    "            modes[k][name].append(metrics[k])\n",
    "        print(modes)\n",
    "        # print(name, decoded_preds[0], decoded_labels[0])\n",
    "        records.append({\"name\": name, \"content\": decoded_preds[0]})\n",
    "        # print(predicted_token_ids[0], labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'system_1',\n",
       "  'content': 'Label: Entailment. Explanation: ItGetting behind a slow driver often leads delays to get angry and it can them late for work work'},\n",
       " {'name': 'system_2',\n",
       "  'content': 'Label: Entailment. Explanation: ItGetting behind a slow driver often leads delays to get angry and it can them late for work work'},\n",
       " {'name': 'system_31',\n",
       "  'content': 'Label: Entailment. Explanation: ItGetting behind a slow driver is leads delays to get angry and it can them late for work work'},\n",
       " {'name': 'system_32',\n",
       "  'content': 'Label: Entailment. Explanation: ItGetting behind a slow driver often leads delays to get angry and it can them late for work work'},\n",
       " {'name': 'system_33',\n",
       "  'content': 'Label: Entailment. Explanation: ItGetting behind a slow driver often leads delays to get angry and it can them late for work work'},\n",
       " {'name': 'system_34',\n",
       "  'content': 'Label: Entailment. Explanation: ItGetting behind a slow driver is leads delays to get angry and it can them late for work work'},\n",
       " {'name': 'system_35',\n",
       "  'content': 'Label: Entailment. Explanation: ItGetting behind a slow driver is leads  to be angry and it can them late for work work and'},\n",
       " {'name': 'system_4',\n",
       "  'content': 'Label: Entailment. Explanation: It is not a good thing to follow a slow driver because it makes someone late for work because it is a sign of frustration and frustration'},\n",
       " {'name': 'system_5',\n",
       "  'content': 'Label: Entailment. Explanation: It is often frustrating when someone follows the slow driver and it makes them late for work because it can lead to delays in the work and so it is natural to feel angry.'},\n",
       " {'name': 'system_6',\n",
       "  'content': 'Label: Entailment. Explanation: ItGetting behind a slow driver is leads frustration to get angry and they can them late for work work'},\n",
       " {'name': 'system_7',\n",
       "  'content': '. Explanation: ItGetting behind a slow driver is leads delays to get angry and it can them late for work work andabel: Entailment'},\n",
       " {'name': 'system_1',\n",
       "  'content': 'Label: Entailment. Explanation: T paper boy  to   and it broke the window and'},\n",
       " {'name': 'system_2',\n",
       "  'content': 'Label: Entailment. Explanation: The paper boy hard to  hard and it broke the window so'},\n",
       " {'name': 'system_31',\n",
       "  'content': 'Label: Entailment. Explanation: T paper boy  hard  hard and it broke the window and'},\n",
       " {'name': 'system_32',\n",
       "  'content': 'Label: Entailment. Explanation: The paper boy  to so  and it broke the window and. Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label'},\n",
       " {'name': 'system_33',\n",
       "  'content': 'Label: Entailment. Explanation: T paper boy  to   and it broke my window and'},\n",
       " {'name': 'system_34',\n",
       "  'content': 'Label: Entailment. Explanation: The paper boy  to so hard and it broke the window and'},\n",
       " {'name': 'system_35',\n",
       "  'content': 'Label: Entailment. Explanation: T paper was  to  hard and it broke my window and'},\n",
       " {'name': 'system_4',\n",
       "  'content': 'Label: Entailment. Explanation: Tripping the newspaper hard is a very embarrassing and unhygienic act and so the paper boy is flung the newspaper and breaking the window.'},\n",
       " {'name': 'system_5',\n",
       "  'content': 'Label: Entailment. Explanation: Tripping the newspaper is a very hard thing to do and so the paper boy is not doing that.'},\n",
       " {'name': 'system_6',\n",
       "  'content': 'Label: Entailment. Explanation: The paper boy so to great  and it broke the window in'},\n",
       " {'name': 'system_7',\n",
       "  'content': '. Explanation: T newspaper boy  very  hard and it broke the window andabel: Entailment'},\n",
       " {'name': 'system_1',\n",
       "  'content': 'Label: Entailment. Explanation: Tinted glass is  transparent, it is d and the fact.'},\n",
       " {'name': 'system_2',\n",
       "  'content': 'Label: Entailment. Explanation: Tinted glass is not transparent, it is d and the glass.'},\n",
       " {'name': 'system_31',\n",
       "  'content': 'Label: Entailment. Explanation: Tinted glass is not transparent, it is d or the glass.'},\n",
       " {'name': 'system_32',\n",
       "  'content': 'Label: Contratailment. Explanation: Tinted glass is  transparent, it is notd and the glass.'},\n",
       " {'name': 'system_33',\n",
       "  'content': 'Label: Entailment. Explanation: Tinted glass is not transparent, it is obscured and the operation of'},\n",
       " {'name': 'system_34',\n",
       "  'content': 'Label: Entailment. Explanation: tintted glass is not transparent, it is d and the glass.'},\n",
       " {'name': 'system_35',\n",
       "  'content': 'Label: Contratailment. Explanation: Tinted glass is  transparent, it is notd and the glass.'},\n",
       " {'name': 'system_4',\n",
       "  'content': 'Label: Entailment. Explanation: tinted glass is not transparent, so the simile is saying that the operation is not transparent.'},\n",
       " {'name': 'system_5',\n",
       "  'content': 'Label: Contradiction. Explanation: A tinted glass is a thin, transparent object, so saying something is as transparent as tinted glass would imply that it is opaque.'},\n",
       " {'name': 'system_6',\n",
       "  'content': 'Label: Entailment. Explanation: tintted glass is not transparent, it is notd and the sunlight of'},\n",
       " {'name': 'system_7',\n",
       "  'content': 'Label: Entailment. Explanation: Tinted glass is not transparent, it is notd and the factL'},\n",
       " {'name': 'ground_truth',\n",
       "  'content': 'Label: Entailment. Explanation: Tinted glass is not transparent because it is obscured by the color.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.append({\"name\": \"ground_truth\", \"content\": decoded_labels[0]})\n",
    "records[-8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n{'system_1': [0.5056115530372459, 0.5085812472965738], 'system_2': [0.5323239453833644, 0.5374525863206762], 'system_31': [0.5084813364883226, 0.5078069259932709], 'system_32': [0.5055255195638098, 0.5074808302929493], 'system_33': [0.5032615710245999, 0.508591734512484], 'system_34': [0.5067396302748032, 0.5087587260751042], 'system_35': [0.5036829298423671, 0.5073656197676405]}\\n{'system_1': [0.47329270980754923], 'system_2': [0.5039333868999851], 'system_31': [0.47420432731446144], 'system_32': [0.473849787491779], 'system_33': [0.47287909150934293], 'system_34': [0.47171914165316353], 'system_35': [0.47436950304876985]}\\n{'system_1': [0.5221458462682274, 0.4906728974226775], 'system_2': [0.5453307495230577, 0.5189864534869755], 'system_31': [0.524029905213074, 0.49432447432649923], 'system_32': [0.5236538569098267, 0.4949640794043393], 'system_33': [0.522696825432352, 0.49418902866839187], 'system_34': [0.5231002650281118, 0.49245360907178504], 'system_35': [0.5238553839735753, 0.4967562420314123]}\\n{'system_1': [0.5201383622010409, 0.5019083214066056], 'system_2': [0.5451744732719431, 0.5273190341222926], 'system_31': [0.5192974336052382, 0.5002995114825706], 'system_32': [0.5188841013023715, 0.5007719063422055], 'system_33': [0.5203219622545853, 0.49992736417041994], 'system_34': [0.5180675680247785, 0.5008015168679895], 'system_35': [0.5224425242712794, 0.5010754435268072]}\\n{'system_1': [0.49590791112913984], 'system_2': [0.5147096144113823], 'system_31': [0.4927430972029428], 'system_32': [0.49258799343269694], 'system_33': [0.49049952341283654], 'system_34': [0.49317378483047924], 'system_35': [0.4919880906784929]}\\n{'system_1': [0.498351050356349, 0.5090844074007058], 'system_2': [0.5289706096196247, 0.535423121872322], 'system_31': [0.5004418699772601, 0.5081924349139884], 'system_32': [0.5000023839289528, 0.5069172466751917], 'system_33': [0.49609349538092806, 0.5060478473679386], 'system_34': [0.49683869684920356, 0.5072891134960673], 'system_35': [0.49873229852851614, 0.5063988604083957]}\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "{'system_1': [0.5056115530372459, 0.5085812472965738], 'system_2': [0.5323239453833644, 0.5374525863206762], 'system_31': [0.5084813364883226, 0.5078069259932709], 'system_32': [0.5055255195638098, 0.5074808302929493], 'system_33': [0.5032615710245999, 0.508591734512484], 'system_34': [0.5067396302748032, 0.5087587260751042], 'system_35': [0.5036829298423671, 0.5073656197676405]}\n",
    "{'system_1': [0.47329270980754923], 'system_2': [0.5039333868999851], 'system_31': [0.47420432731446144], 'system_32': [0.473849787491779], 'system_33': [0.47287909150934293], 'system_34': [0.47171914165316353], 'system_35': [0.47436950304876985]}\n",
    "{'system_1': [0.5221458462682274, 0.4906728974226775], 'system_2': [0.5453307495230577, 0.5189864534869755], 'system_31': [0.524029905213074, 0.49432447432649923], 'system_32': [0.5236538569098267, 0.4949640794043393], 'system_33': [0.522696825432352, 0.49418902866839187], 'system_34': [0.5231002650281118, 0.49245360907178504], 'system_35': [0.5238553839735753, 0.4967562420314123]}\n",
    "{'system_1': [0.5201383622010409, 0.5019083214066056], 'system_2': [0.5451744732719431, 0.5273190341222926], 'system_31': [0.5192974336052382, 0.5002995114825706], 'system_32': [0.5188841013023715, 0.5007719063422055], 'system_33': [0.5203219622545853, 0.49992736417041994], 'system_34': [0.5180675680247785, 0.5008015168679895], 'system_35': [0.5224425242712794, 0.5010754435268072]}\n",
    "{'system_1': [0.49590791112913984], 'system_2': [0.5147096144113823], 'system_31': [0.4927430972029428], 'system_32': [0.49258799343269694], 'system_33': [0.49049952341283654], 'system_34': [0.49317378483047924], 'system_35': [0.4919880906784929]}\n",
    "{'system_1': [0.498351050356349, 0.5090844074007058], 'system_2': [0.5289706096196247, 0.535423121872322], 'system_31': [0.5004418699772601, 0.5081924349139884], 'system_32': [0.5000023839289528, 0.5069172466751917], 'system_33': [0.49609349538092806, 0.5060478473679386], 'system_34': [0.49683869684920356, 0.5072891134960673], 'system_35': [0.49873229852851614, 0.5063988604083957]}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This operation is obscured and not transparent. ',\n",
       " 'This operation is as transparent as tinted glass. Is there a contradiction or entailment between the premise and hypothesis?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system_1': [0.6544642734433673, 0.6538472479091643, 0.6451995135621926],\n",
       " 'system_2': [0.6591784410473625, 0.6556348348860563, 0.6485904916673478],\n",
       " 'system_31': [0.6616151178011304, 0.6529529681168658, 0.6457822724848103],\n",
       " 'system_32': [0.6602817794390899, 0.6454009215237158, 0.6454316035156613],\n",
       " 'system_33': [0.6577626817560993, 0.654545940696381, 0.6495673748102144],\n",
       " 'system_34': [0.6577570152042305, 0.6535457188788231, 0.6434608160222891],\n",
       " 'system_35': [0.6555744419703384, 0.6538046517382146, 0.6470891971959015],\n",
       " 'system_4': [0.52284036210826, 0.5163325460813661, 0.5156399803184772],\n",
       " 'system_5': [0.5240891672641392, 0.5080151274468501, 0.5057846437218605],\n",
       " 'system_6': [0.6635674632696552, 0.6627904866947751, 0.6585561479873518],\n",
       " 'system_7': [0.6611524853455227, 0.6590132359931218, 0.6561421023328687]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system_2 TtestResult(statistic=23.19102411438574, pvalue=1.2259514290288666e-09, df=9)\n",
      "system_31 TtestResult(statistic=0.5948201499859518, pvalue=0.2833078171850142, df=9)\n",
      "system_32 TtestResult(statistic=-0.15212858962856382, pvalue=0.5587793758146149, df=9)\n",
      "system_33 TtestResult(statistic=-1.4525349538900345, pvalue=0.9098456440410233, df=9)\n",
      "system_34 TtestResult(statistic=-1.3719083987600567, pvalue=0.898341997218993, df=9)\n",
      "system_35 TtestResult(statistic=0.10660074133722681, pvalue=0.45872204141041606, df=9)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# paired t test\n",
    "for mode in modes:\n",
    "    print(mode)\n",
    "    for key in list(modes[mode].keys())[1:]:\n",
    "        print(\"---\", key, ttest_rel(modes[mode][key], modes[mode][\"system_1\"], alternative=\"greater\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
