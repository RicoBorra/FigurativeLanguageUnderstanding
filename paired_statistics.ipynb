{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 12\n",
    "NUM_EPOCHS = 1\n",
    "N_GEN = 50\n",
    "base_checkpoint = \"t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_checkpoint)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ds = load_dataset(\"ColumbiaNLP/FLUTE\").shuffle(seed=42)\n",
    "df = pd.read_csv(\"complete_dataset.csv\").fillna(\"\")\n",
    "ds = Dataset.from_pandas(df).shuffle(seed=42)\n",
    "folds = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "splits = folds.split(ds, ds[\"label\"])\n",
    "indexes = [t for t in splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ccea5d79c044dc80e3c23907ba5cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7534 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from flute_dream import add_combined_cols\n",
    "\n",
    "ds = ds.map(add_combined_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_s1(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis\"])\n",
    "    labels = tokenizer(examples[\"label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s2(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_system_2\"])\n",
    "    labels = tokenizer(examples[\"type_label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s31(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_emotion\"])\n",
    "    labels = tokenizer(examples[\"label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s32(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_motivation\"])\n",
    "    labels = tokenizer(examples[\"label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s33(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_consequence\"])\n",
    "    labels = tokenizer(examples[\"label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s34(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_rot\"])\n",
    "    labels = tokenizer(examples[\"label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s35(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_all_dims\"])\n",
    "    labels = tokenizer(examples[\"label_explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s41(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis\"])\n",
    "    labels = tokenizer(examples[\"label\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s42(examples):\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis_label\"])\n",
    "    labels = tokenizer(examples[\"explanation\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset_s7(examples):\n",
    "    lbl_exp = [\n",
    "        ex[ex.find(\"Explanation: \") :] + ex[: ex.find(\"Explanation: \")]\n",
    "        for ex in examples[\"label_explanation\"]\n",
    "    ]\n",
    "    model_inputs = tokenizer(examples[\"premise_hypothesis\"])\n",
    "    labels = tokenizer(lbl_exp)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def get_path(name):\n",
    "    return f\"{name}/{os.listdir(name)[0]}\"\n",
    "\n",
    "\n",
    "def get_prem_hyp(s):\n",
    "    ind = s.find(\"Hypothesis: \")\n",
    "    prem = s[len(\"Premise: \") : ind]\n",
    "    hyp = s[ind + len(\"Hypothesis: \") :]\n",
    "    return [prem, hyp]\n",
    "\n",
    "\n",
    "\"\"\"Class encapsulating the two steps of System 4 (Classify, then Explain)\"\"\"\n",
    "\n",
    "\n",
    "class DREAM_FLUTE_System4:\n",
    "    def __init__(\n",
    "        self, tokenizer=None, model_s41_path=None, model_s42_path=None\n",
    "    ) -> None:\n",
    "        self.tokenizer = (\n",
    "            tokenizer\n",
    "            if tokenizer is not None\n",
    "            else AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "        )\n",
    "        self.model_s41 = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(model_s41_path)\n",
    "            if model_s41_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                \"YoanBOUTE/DREAM-FLUTE-S4-Classify\"\n",
    "            )\n",
    "        )\n",
    "        self.model_s42 = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(model_s42_path)\n",
    "            if model_s42_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                \"YoanBOUTE/DREAM-FLUTE-S4-Explain\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    \"\"\"Expected input for function : \"Premise: ... . Hypothesis: ... . Is there a contradiction or entailment between the premise and hypothesis ?\" \n",
    "    Or list of strings in this format\"\"\"\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        conc_inp = [in1 + in2 for in1, in2 in inputs]\n",
    "        self.model_s41 = self.model_s41.to(device)\n",
    "        self.model_s42 = self.model_s42.to(device)\n",
    "        tok_input = self.tokenizer(conc_inp, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            output_model_1 = self.model_s41.generate(tok_input, max_new_tokens=100)\n",
    "        labels = [\"Label: \" + el for el in self.tokenizer.batch_decode(output_model_1, skip_special_tokens=True)]\n",
    "        s1 = \"Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        s2 = \". What is the explanation of the label associated to the premise and the hypothesis ?\"\n",
    "        intermediate_input = [inp[:inp.find(s1)] + lbl + s2 for inp, lbl in zip(conc_inp, labels)]\n",
    "        tok_intermediate_input = self.tokenizer(intermediate_input, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            output_model_2 = self.model_s42.generate(tok_intermediate_input, max_new_tokens=100)\n",
    "        explanations = self.tokenizer.batch_decode(output_model_2, skip_special_tokens=True)\n",
    "        return [lbl + \". Explanation: \" + expl for lbl, expl in zip(labels, explanations)]\n",
    "    \n",
    "    \n",
    "    def prediction_pipeline(self, inputs):\n",
    "        if isinstance(inputs, str):\n",
    "            tok_input = self.tokenizer(inputs, return_tensors=\"pt\").input_ids\n",
    "            output_model_1 = self.model_s41.generate(tok_input, max_new_tokens=100)\n",
    "            decoded_output_model_1 = \"Label: \" + self.tokenizer.decode(\n",
    "                output_model_1[0], skip_special_tokens=True\n",
    "            )\n",
    "            intermediate_input = (\n",
    "                inputs[\n",
    "                    : inputs.find(\n",
    "                        \"Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "                    )\n",
    "                ]\n",
    "                + decoded_output_model_1\n",
    "                + \". What is the explanation of the label associated to the premise and the hypothesis ?\"\n",
    "            )\n",
    "            tok_intermediate_input = self.tokenizer(\n",
    "                intermediate_input, return_tensors=\"pt\"\n",
    "            ).input_ids\n",
    "            output_model_2 = self.model_s42.generate(\n",
    "                tok_intermediate_input, max_new_tokens=100\n",
    "            )\n",
    "\n",
    "            return (\n",
    "                decoded_output_model_1\n",
    "                + \". Explanation: \"\n",
    "                + self.tokenizer.decode(output_model_2[0], skip_special_tokens=True)\n",
    "            )\n",
    "\n",
    "        elif isinstance(inputs, list) and all(\n",
    "            isinstance(input, str) for input in inputs\n",
    "        ):\n",
    "            predictions = []\n",
    "            for input in inputs:\n",
    "                predictions.append(self.prediction_pipeline(input))\n",
    "\n",
    "            return predictions\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"Inputs should be either a list of two strings or a list of lists of two strings\"\n",
    "            )\n",
    "\n",
    "\n",
    "\"\"\"Ensemble class that loads all models from HuggingFace (or from the device if a path to the model is indicated) \n",
    "and implements the ensembling algorithm given in the DREAM-FLUTE paper\"\"\"\n",
    "\n",
    "\n",
    "class DREAM_FLUTE_Ensemble:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer_path=None,\n",
    "        s1_path=None,\n",
    "        s2_path=None,\n",
    "        s3_emo_path=None,\n",
    "        s3_mot_path=None,\n",
    "        s3_cons_path=None,\n",
    "        s3_rot_path=None,\n",
    "        s3_alldims_path=None,\n",
    "        s4_clas_path=None,\n",
    "        s4_exp_path=None,\n",
    "        dream_path=None,\n",
    "    ) -> None:\n",
    "        self.tokenizer = (\n",
    "            AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "            if tokenizer_path is not None\n",
    "            else AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "        )\n",
    "        self.model_s1 = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s1_path)\n",
    "            if s1_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\"YoanBOUTE/DREAM-FLUTE-S1\")\n",
    "        )\n",
    "        self.model_s2 = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s2_path)\n",
    "            if s2_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\"YoanBOUTE/DREAM-FLUTE-S2\")\n",
    "        )\n",
    "        self.model_s3_emo = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s3_emo_path)\n",
    "            if s3_emo_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                \"YoanBOUTE/DREAM-FLUTE-S3-Emotion\"\n",
    "            )\n",
    "        )\n",
    "        self.model_s3_mot = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s3_mot_path)\n",
    "            if s3_mot_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                \"YoanBOUTE/DREAM-FLUTE-S3-Motivation\"\n",
    "            )\n",
    "        )\n",
    "        self.model_s3_cons = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s3_cons_path)\n",
    "            if s3_cons_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                \"YoanBOUTE/DREAM-FLUTE-S3-Consequence\"\n",
    "            )\n",
    "        )\n",
    "        self.model_s3_rot = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s3_rot_path)\n",
    "            if s3_rot_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\"YoanBOUTE/DREAM-FLUTE-S3-ROT\")\n",
    "        )\n",
    "        self.model_s3_alldims = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(s3_alldims_path)\n",
    "            if s3_alldims_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                \"YoanBOUTE/DREAM-FLUTE-S3-AllDims\"\n",
    "            )\n",
    "        )\n",
    "        self.model_s4 = DREAM_FLUTE_System4(self.tokenizer, s4_clas_path, s4_exp_path)\n",
    "        self.model_dream = (\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(dream_path)\n",
    "            if dream_path is not None\n",
    "            else AutoModelForSeq2SeqLM.from_pretrained(\"RicoBorra/DREAM-t5-small\")\n",
    "        )\n",
    "\n",
    "    \"\"\"Tokenizes the input, then feeds it to the given model, and decodes the output to have a string as result.\n",
    "    This method is callable for all models except System 4 (Use the method defined in the class of System 4)\"\"\"\n",
    "\n",
    "    def _prediction_pipeline(self, input: str, model) -> str:\n",
    "        tokenized_input = self.tokenizer(input, return_tensors=\"pt\").input_ids\n",
    "        model_output = model.generate(tokenized_input, max_new_tokens=100)\n",
    "        decoded_output = self.tokenizer.decode(\n",
    "            model_output[0], skip_special_tokens=True\n",
    "        )\n",
    "        return decoded_output\n",
    "\n",
    "    \"\"\"Preprocesses the input for each model, then feeds it to the pipeline.\n",
    "    Returns a dictionary of all models' predictions.\"\"\"\n",
    "\n",
    "    def _get_all_predictions(self, input: list):\n",
    "        prem, hyp = input\n",
    "        prem = prem.strip()\n",
    "        hyp = hyp.strip()\n",
    "        if not prem.endswith(\".\"):\n",
    "            prem += \".\"\n",
    "        if not hyp.endswith(\".\"):\n",
    "            hyp += \".\"\n",
    "\n",
    "        predictions = dict()\n",
    "\n",
    "        input_1 = f\"Premise: {prem} Hypothesis: {hyp} Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S1\"] = self._prediction_pipeline(input_1, self.model_s1)\n",
    "\n",
    "        input_2 = f\"Premise: {prem} Hypothesis: {hyp} What is the type of figurative language involved? Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S2\"] = self._prediction_pipeline(input_2, self.model_s2)\n",
    "\n",
    "        # DREAM elaborations for system 3\n",
    "        input_dream_prem = f\"[SITUATION] {prem} [QUERY] \"\n",
    "        input_dream_hyp = f\"[SITUATION] {hyp} [QUERY] \"\n",
    "        prem_elaborations = {\n",
    "            key: self._prediction_pipeline(input_dream_prem + key, self.model_dream)\n",
    "            for key in [\"emotion\", \"motivation\", \"consequence\", \"rot\"]\n",
    "        }\n",
    "        for key, elab in prem_elaborations.items():\n",
    "            elab = elab.strip()\n",
    "            if not elab.endswith(\".\"):\n",
    "                prem_elaborations[key] += \".\"\n",
    "        hyp_elaborations = {\n",
    "            key: self._prediction_pipeline(input_dream_hyp + key, self.model_dream)\n",
    "            for key in [\"emotion\", \"motivation\", \"consequence\", \"rot\"]\n",
    "        }\n",
    "        for key, elab in hyp_elaborations.items():\n",
    "            elab = elab.strip()\n",
    "            if not elab.endswith(\".\"):\n",
    "                hyp_elaborations[key] += \".\"\n",
    "\n",
    "        input_3_emo = f\"Premise: {prem} [Emotion] {prem_elaborations['emotion']} Hypothesis: {hyp} [Emotion] {hyp_elaborations['emotion']} Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S3-emo\"] = self._prediction_pipeline(\n",
    "            input_3_emo, self.model_s3_emo\n",
    "        )\n",
    "\n",
    "        input_3_mot = f\"Premise: {prem} [Motivation] {prem_elaborations['motivation']} Hypothesis: {hyp} [Motivation] {hyp_elaborations['motivation']} Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S3-mot\"] = self._prediction_pipeline(\n",
    "            input_3_mot, self.model_s3_mot\n",
    "        )\n",
    "\n",
    "        input_3_cons = f\"Premise: {prem} [Consequence] {prem_elaborations['consequence']} Hypothesis: {hyp} [Consequence] {hyp_elaborations['consequence']} Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S3-cons\"] = self._prediction_pipeline(\n",
    "            input_3_cons, self.model_s3_cons\n",
    "        )\n",
    "\n",
    "        input_3_rot = f\"Premise: {prem} [Rot] {prem_elaborations['rot']} Hypothesis: {hyp} [Rot] {hyp_elaborations['rot']} Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S3-rot\"] = self._prediction_pipeline(\n",
    "            input_3_rot, self.model_s3_rot\n",
    "        )\n",
    "\n",
    "        input_3_all = f\"Premise: {prem} \"\n",
    "        for key, elab in prem_elaborations.items():\n",
    "            input_3_all += f\"[{key.capitalize()}] {elab} \"\n",
    "        input_3_all += f\"Hypothesis: {hyp} \"\n",
    "        for key, elab in hyp_elaborations.items():\n",
    "            input_3_all += f\"[{key.capitalize()}] {elab} \"\n",
    "        input_3_all += \"Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions[\"S3-all\"] = self._prediction_pipeline(\n",
    "            input_3_all, self.model_s3_alldims\n",
    "        )\n",
    "\n",
    "        # The input for system 4 is in the same format as for system 1\n",
    "        predictions[\"S4\"] = self.model_s4.prediction_pipeline(input_1)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    \"\"\"Uses the predictions from each model to compute the final prediction of the ensemble\"\"\"\n",
    "\n",
    "    def _ensemble_algorithm(self, model_outputs):\n",
    "        # Firstly, the label is selected based on the majority between the 5 best models (according to the paper : systems 1, 2, 3-motivation, 3-alldims, 4)\n",
    "        labels = [\n",
    "            model_outputs[key].split(\".\")[0]\n",
    "            for key in [\"S1\", \"S2\", \"S3-mot\", \"S3-all\", \"S4\"]\n",
    "        ]\n",
    "        # Sometimes, it might happen with the small models that the generated label is a mix of words, like 'Contratailment' or 'Endiction'\n",
    "        for label in labels:\n",
    "            if label not in [\"Label: Contradiction\", \"Label: Entailment\"]:\n",
    "                labels.remove(label)\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        ix = np.argmax(counts)\n",
    "        major_label = unique[ix]\n",
    "\n",
    "        # Then, pick the explanation of the first system agreeing with the majority label, following an order indicated in the paper\n",
    "        for key in [\"S3-cons\", \"S3-emo\", \"S2\", \"S3-all\", \"S3-mot\", \"S4\", \"S1\"]:\n",
    "            substrings = model_outputs[key].split(\".\")\n",
    "            label = substrings[0]\n",
    "            explanation = substrings[1]\n",
    "\n",
    "            if label == major_label:\n",
    "                break\n",
    "\n",
    "        return major_label + \".\" + explanation + \".\"\n",
    "\n",
    "    \"\"\"Expected input : [Premise_sentence, hypothesis_sentence] or list of inputs\"\"\"\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        if (\n",
    "            isinstance(inputs, list)\n",
    "            and all(isinstance(input, str) for input in inputs)\n",
    "            and len(inputs) == 2\n",
    "        ):\n",
    "            preds = self._get_all_predictions(inputs)\n",
    "            final_pred = self._ensemble_algorithm(preds)\n",
    "\n",
    "            return final_pred\n",
    "\n",
    "        elif isinstance(inputs, list) and all(\n",
    "            isinstance(input, list) for input in inputs\n",
    "        ):\n",
    "            predictions = []\n",
    "            for input in inputs:\n",
    "                predictions.append(self.predict(input))\n",
    "\n",
    "            return predictions\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"Inputs should be either a list of two strings or a list of lists of two strings\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "operating_modes = [\n",
    "    (\"system_1\", preprocess_dataset_s1),\n",
    "    (\"system_2\", preprocess_dataset_s2),\n",
    "    (\"system_31\", preprocess_dataset_s31),\n",
    "    (\"system_32\", preprocess_dataset_s32),\n",
    "    (\"system_33\", preprocess_dataset_s33),\n",
    "    (\"system_34\", preprocess_dataset_s34),\n",
    "    (\"system_35\", preprocess_dataset_s35),\n",
    "    (\"system_4\", preprocess_dataset_s1),\n",
    "    (\"system_5\", preprocess_dataset_s1),\n",
    "    (\"system_6\", preprocess_dataset_s1),\n",
    "    (\"system_7\", preprocess_dataset_s7),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(name, model, curr_ds):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"{name}\",\n",
    "        learning_rate=3e-4,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=2 * BATCH_SIZE,\n",
    "        save_total_limit=1,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        report_to=\"none\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        eval_accumulation_steps=1,\n",
    "        logging_steps=1,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=curr_ds[\"train\"],\n",
    "        eval_dataset=curr_ds[\"val\"].select(range(350)),\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('system_4', <function __main__.preprocess_dataset_s1(examples)>),\n",
       " ('system_5', <function __main__.preprocess_dataset_s1(examples)>),\n",
       " ('system_6', <function __main__.preprocess_dataset_s1(examples)>),\n",
       " ('system_7', <function __main__.preprocess_dataset_s7(examples)>)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "operating_modes = operating_modes[-4:]\n",
    "operating_modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to predict\n",
      "predicting\n",
      "['I really do not like when I order a hamburger with no ketchup and they still put it on. ', 'I am elated that the super attentive restaurant staff did not listen to my order and put ketchup on my hamburger. Is there a contradiction or entailment between the premise and hypothesis?']\n",
      "device cuda\n",
      "tokenized\n",
      "torch.Size([50, 86])\n",
      "first output\n"
     ]
    }
   ],
   "source": [
    "sys_4 = DREAM_FLUTE_System4(\n",
    "    tokenizer=None,\n",
    "    model_s41_path=get_path(\"system_41\"),\n",
    "    model_s42_path=get_path(\"system_42\"),\n",
    ")\n",
    "print(\"about to predict\")\n",
    "decoded_preds = sys_4.predict(inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Label: Contradiction. Explanation: The restaurant staff are not attentive and not attentive to the order and not listening to it is not a good thing to do.',\n",
       " 'Label: Contradiction. Explanation: Being busy working is often a very stressful experience and so having not been able to make more time for friends is not a good thing.',\n",
       " 'Label: Contradiction. Explanation: To be absorbed means to be absorbed, but in this context the organization is being independent.',\n",
       " 'Label: Contradiction. Explanation: A london fog is a very thin, thin, and thin fog.',\n",
       " 'Label: Contradiction. Explanation: To bring happiness and joy into a life means to bring happiness and joy into the world, but in this context she brought happiness and joy into the world.',\n",
       " 'Label: Contradiction. Explanation: A roommate is not a good thing and so it is not a good thing to do and so it is not a good thing to do.',\n",
       " 'Label: Contradiction. Explanation: To attract something means to have a gift, but in this context the person is not a gift.',\n",
       " 'Label: Contradiction. Explanation: McDonalds is a food that is not a will power and so someone who is responsible and proud about succumbing to their will power is not responsible or proud.',\n",
       " 'Label: Entailment. Explanation: Living alone in a rough neighborhood is often a very stressful experience and so feeling scared about something bad is not a contradiction.',\n",
       " 'Label: Entailment. Explanation: Middle age is a very difficult time to travel, so the person is in their middle age and hence she lost her zest for adventure.',\n",
       " 'Label: Contradiction. Explanation: To be a two-way street means to be a reciprocal situation, but in this context the parties are required to take action.',\n",
       " 'Label: Entailment. Explanation: To be young blood means to be vibrant and enthusiastic, but in this context the man is fresh, vibrant and enthusiastic.',\n",
       " \"Label: Contradiction. Explanation: A bank charged someone for a credit card that they didn't want is not a good thing and so someone who is thrilled about it is not a good thing.\",\n",
       " 'Label: Contradiction. Explanation: To go ballistic on someone means to go ballistic, but in this context the speaker is likely to be level-headed.',\n",
       " 'Label: Contradiction. Explanation: A job interview that is often a very stressful experience and so feeling excited about it is not a contradiction.',\n",
       " 'Label: Contradiction. Explanation: To forget something to something is to be ashamed of it, but in this context the speaker is not ashamed of it.',\n",
       " 'Label: Contradiction. Explanation: A cavern is a small space, so the space that contained it was small is not a neutral space.',\n",
       " 'Label: Contradiction. Explanation: To shuffle off this mortal coil means to shuffle off the mortal coil, but in this context the person is trying to stay alive and not die.',\n",
       " 'Label: Contradiction. Explanation: To be singly means to be prevalent only one at a time, but in this context disasters are not prevalent.',\n",
       " 'Label: Contradiction. Explanation: A thunderstorm is a very dangerous event and so someone who is brave is not brave.',\n",
       " 'Label: Contradiction. Explanation: A girl that works at the gas station is not a good thing and so someone who is excited and happy about it is not a good thing.',\n",
       " 'Label: Entailment. Explanation: It is often a very bad experience to buy something that is not really helpful to someone and so someone who is not able to help them out when they need it is not a contradiction.',\n",
       " 'Label: Entailment. Explanation: A dried flower is a flower that is pliably shaped, so the speaker is saying that the speaker is pliably positioned.',\n",
       " 'Label: Contradiction. Explanation: A team that is a big lead in the playoffs is a very strong team and so if the team is a big lead, it would be a contradiction.',\n",
       " \"Label: Contradiction. Explanation: Seeing someone's boyfriend for two days is not a good thing and so feeling loved when it happens is not a good thing.\",\n",
       " 'Label: Entailment. Explanation: It is not a good thing to play video games instead of cleaning the house like a wife asked to because it is a bad thing and so playing video games instead of cleaning the house like a wife would be a bad thing.',\n",
       " 'Label: Contradiction. Explanation: A hurricane is a hurricane, so the dancer is as graceful as a hurricane.',\n",
       " \"Label: Contradiction. Explanation: A dull personality is a very unpleasant experience, so the woman's personality is as intriguing as oatmeal.\",\n",
       " 'Label: Contradiction. Explanation: To leave it at that means to stop or take further action, but in this context the person is not doing enough to stop there and spend more time taking further action.',\n",
       " 'Label: Contradiction. Explanation: Being caught off guard when someone is complaining about an annoying client is not a good thing and so seeing it sitting outside the door is not a good thing.',\n",
       " 'Label: Contradiction. Explanation: To spoil the broth means to make the final product better, but in this context the cooks are trying to do something that is not good.',\n",
       " \"Label: Contradiction. Explanation: A new motorcycle is a very dangerous vehicle and so someone who crashes it into a neighbour's car is not being careful and is not being careful.\",\n",
       " 'Label: Contradiction. Explanation: To be simple and easy to understand means to be simple and easy to understand, but in this context the speaker is saying that the relationship between the two is not simple and easy to understand.',\n",
       " 'Label: Contradiction. Explanation: A clean plate at the bottom of a scummy sink is a very small, scummy sink, so the opposite is not true.',\n",
       " \"Label: Contradiction. Explanation: It is not a good thing to be excited about someone's mother getting old and not good health, but rather a bad thing.\",\n",
       " 'Label: Entailment. Explanation: A shadow is a shadow, so the entailment is a contradiction.',\n",
       " \"Label: Contradiction. Explanation: A daughter's boyfriend is not a violent and friendly person, so if the person is not a violent person, it would be a contradiction.\",\n",
       " 'Label: Contradiction. Explanation: To run amok means to run amok, but in this context the imagination is stayed in check.',\n",
       " 'Label: Contradiction. Explanation: A big baby is a very mature child, so if the brother is mature, it would be a very sarcastic and sarcastic act.',\n",
       " 'Label: Entailment. Explanation: To give up the ghost means to stop working and die, but in this context the batteries are about to stop working and die.',\n",
       " 'Label: Contradiction. Explanation: A broken bridge is a broken bridge, so the building is not strong.',\n",
       " 'Label: Contradiction. Explanation: To flick away something means to give it a new-found hope, but in this context she gave the person a new-found hope.',\n",
       " 'Label: Entailment. Explanation: A swamp cat is a cat that is a very dangerous animal, so the person is running, hard and reckless.',\n",
       " 'Label: Entailment. Explanation: A 15 year old son crashed his ferrari while taking it for a test drive is a very dangerous thing and so the son is upset about it.',\n",
       " 'Label: Contradiction. Explanation: To embrace something means to embrace it, but in this context the person is saying that they should feel ashamed.',\n",
       " 'Label: Entailment. Explanation: To be in complete disarray means to be in complete disarray, but in this context everything is in complete disarray and not working as intended.',\n",
       " 'Label: Contradiction. Explanation: Smoking a cigarrete and the ash drops in your work pants and leaves a stain is not a good thing because it is a good thing to do and so the person is not loving.',\n",
       " 'Label: Contradiction. Explanation: The carnal screams of a rock concert moshpit are a very loud and loud sound, so the sound hit Frank like the carnal screams of a rock concert moshpit is not a contradiction.',\n",
       " 'Label: Contradiction. Explanation: To flow to his glory means to flow to his glory, but in this context the speaker is saying that everything flowed to his glory.',\n",
       " 'Label: Contradiction. Explanation: Running errands is a very stressful and stressful day and so finding someone broke in and taking a tv and laptop is not a good thing.']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6771bf49dbf0492e9b6800ba491aeff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6781 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c1b72e2fd64312aca95134acff256d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/753 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a45712c3a842f2940b2df34e848a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6781 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ffdf16a2974b0e886dd24368b0495f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/753 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19028/1163061896.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m                         \u001b[0mdream_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                     )\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mdecoded_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys_5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;31m# careful here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_19028/3951511381.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                 \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_19028/3951511381.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         ):\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_all_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mfinal_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensemble_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_19028/3951511381.py\u001b[0m in \u001b[0;36m_get_all_predictions\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0minput_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Premise: {prem} Hypothesis: {hyp} What is the type of figurative language involved? Is there a contradiction or entailment between the premise and hypothesis ?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"S2\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prediction_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_s2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;31m# DREAM elaborations for system 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_19028/3951511381.py\u001b[0m in \u001b[0;36m_prediction_pipeline\u001b[0;34m(self, input, model)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prediction_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mtokenized_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         decoded_output = self.tokenizer.decode(\n\u001b[1;32m    264\u001b[0m             \u001b[0mmodel_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1716\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgeneration_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGREEDY_SEARCH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1717\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1718\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m   1719\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2578\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2579\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2580\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2581\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1744\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1108\u001b[0m                 )\n\u001b[1;32m   1109\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1111\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;31m# Apply Feed Forward layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;31m# clamp inf values to enable fp16 training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseReluDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# half-precision inputs is done in fp32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "modes = {t: {name: [] for name, _ in operating_modes} for t in ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']}\n",
    "\n",
    "for train_idxs, val_idxs in indexes[4:7]:\n",
    "    fold_dataset = DatasetDict(\n",
    "        {\"train\": ds.select(train_idxs), \"val\": ds.select(val_idxs)}\n",
    "    )\n",
    "\n",
    "    for name, preprocess_func in operating_modes:\n",
    "        curr_ds = fold_dataset.map(preprocess_func, batched=True).remove_columns(\n",
    "            fold_dataset[\"train\"].column_names\n",
    "        )\n",
    "\n",
    "        if name == \"system_4\":\n",
    "            continue\n",
    "            ds_41 = fold_dataset.map(\n",
    "                preprocess_dataset_s41, batched=True\n",
    "            ).remove_columns(fold_dataset[\"train\"].column_names)\n",
    "            train_model(\n",
    "                \"system_41\",\n",
    "                AutoModelForSeq2SeqLM.from_pretrained(base_checkpoint),\n",
    "                ds_41,\n",
    "            )\n",
    "            ds_42 = fold_dataset.map(\n",
    "                preprocess_dataset_s42, batched=True\n",
    "            ).remove_columns(fold_dataset[\"train\"].column_names)\n",
    "            train_model(\n",
    "                \"system_42\",\n",
    "                AutoModelForSeq2SeqLM.from_pretrained(base_checkpoint),\n",
    "                ds_42,\n",
    "            )\n",
    "        elif name == \"system_5\":\n",
    "            pass\n",
    "        else:\n",
    "            if name == \"system_6\":\n",
    "                model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    \"RicoBorra/T5-small-synthetic-FLUTE\"\n",
    "                )\n",
    "            else:\n",
    "                model = AutoModelForSeq2SeqLM.from_pretrained(base_checkpoint)\n",
    "            trainer = train_model(name, model, curr_ds)\n",
    "\n",
    "        # have to do batched rouge computation otherwise not enough memory\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        metrics = {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0}\n",
    "        count = 0\n",
    "        for i in range(0, len(curr_ds[\"val\"]), N_GEN):\n",
    "            count += 1\n",
    "            if (name != \"system_4\") and (name != \"system_5\"):\n",
    "                (predictions, _), label_ids, _ = trainer.predict(\n",
    "                    test_dataset=curr_ds[\"val\"].select(\n",
    "                        range(i, min(i + N_GEN, len(curr_ds[\"val\"])))\n",
    "                    )\n",
    "                )\n",
    "                # delete stuff after EOS token\n",
    "                predicted_token_ids = torch.argmax(\n",
    "                    torch.from_numpy(predictions), dim=-1\n",
    "                )\n",
    "                for j in range(predicted_token_ids.shape[0]):\n",
    "                    ind = (predicted_token_ids[j] == 1).nonzero(as_tuple=True)[0]\n",
    "                    if ind.numel() != 0:\n",
    "                        predicted_token_ids[j, ind[0] :] = 1\n",
    "                decoded_preds = tokenizer.batch_decode(\n",
    "                    predicted_token_ids, skip_special_tokens=True\n",
    "                )\n",
    "                # clean decoded preds if needed\n",
    "                if name == \"system_2\":\n",
    "                    decoded_preds = [dp[dp.find(\"Label\") :] for dp in decoded_preds]\n",
    "                if name == \"system_7\":\n",
    "                    decoded_preds = [\n",
    "                        ex[ex.find(\"Label: \") :] + \" \" + ex[: ex.find(\"Label: \")]\n",
    "                        for ex in decoded_preds\n",
    "                    ]\n",
    "\n",
    "            else:\n",
    "                small_ds = curr_ds[\"val\"].select(\n",
    "                    range(i, min(i + N_GEN, len(curr_ds[\"val\"])))\n",
    "                )\n",
    "                label_ids = small_ds[\"labels\"]\n",
    "                max_len = max([len(el) for el in label_ids])\n",
    "                label_ids = [el + (max_len - len(el)) * [0] for el in label_ids]\n",
    "                inputs = tokenizer.batch_decode(\n",
    "                    small_ds[\"input_ids\"], skip_special_tokens=True\n",
    "                )\n",
    "                if name == \"system_4\":\n",
    "                    sys_4 = DREAM_FLUTE_System4(\n",
    "                        tokenizer=None,\n",
    "                        model_s41_path=get_path(\"system_41\"),\n",
    "                        model_s42_path=get_path(\"system_42\"),\n",
    "                    )\n",
    "                    print(\"about to predict\")\n",
    "                    decoded_preds = sys_4.predict(inputs=inputs)\n",
    "                else:  # \"name == system_5\"\n",
    "                    inputs = [get_prem_hyp(ex) for ex in inputs]\n",
    "                    sys_5 = DREAM_FLUTE_Ensemble(\n",
    "                        tokenizer_path=None,\n",
    "                        s1_path=get_path(\"system_1\"),\n",
    "                        s2_path=get_path(\"system_2\"),\n",
    "                        s3_emo_path=get_path(\"system_31\"),\n",
    "                        s3_mot_path=get_path(\"system_32\"),\n",
    "                        s3_cons_path=get_path(\"system_33\"),\n",
    "                        s3_rot_path=get_path(\"system_34\"),\n",
    "                        s3_alldims_path=get_path(\"system_35\"),\n",
    "                        s4_clas_path=get_path(\"system_41\"),\n",
    "                        s4_exp_path=get_path(\"system_42\"),\n",
    "                        dream_path=None,\n",
    "                    )\n",
    "                    decoded_preds = sys_5.predict(inputs=inputs)\n",
    "\n",
    "            # careful here\n",
    "            labels = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            if name == \"system_2\":\n",
    "                decoded_labels = [dp[dp.find(\"Label\") :] for dp in decoded_labels]\n",
    "            if name == \"system_7\":\n",
    "                decoded_labels = [\n",
    "                    ex[ex.find(\"Label: \") :] + \" \" + ex[: ex.find(\"Label: \")]\n",
    "                    for ex in decoded_labels\n",
    "                ]\n",
    "\n",
    "            new_metrics = rouge.compute(\n",
    "                predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "            )\n",
    "            for k in new_metrics:\n",
    "                metrics[k] += new_metrics[k]\n",
    "\n",
    "        for k in metrics:\n",
    "            metrics[k] /= count\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        for k in metrics:\n",
    "            modes[k][name].append(metrics[k])\n",
    "        print(modes)\n",
    "        # print(name, decoded_preds[0], decoded_labels[0])\n",
    "        records.append({\"name\": name, \"content\": decoded_preds[0]})\n",
    "        # print(predicted_token_ids[0], labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'system_1',\n",
       "  'content': 'Label: Entailment. Explanation: ItGetting behind a slow driver often leads delays to get angry and it can them late for work work'},\n",
       " {'name': 'system_2',\n",
       "  'content': 'Label: Entailment. Explanation: ItGetting behind a slow driver often leads delays to get angry and it can them late for work work'},\n",
       " {'name': 'system_31',\n",
       "  'content': 'Label: Entailment. Explanation: ItGetting behind a slow driver is leads delays to get angry and it can them late for work work'},\n",
       " {'name': 'system_32',\n",
       "  'content': 'Label: Entailment. Explanation: ItGetting behind a slow driver often leads delays to get angry and it can them late for work work'},\n",
       " {'name': 'system_33',\n",
       "  'content': 'Label: Entailment. Explanation: ItGetting behind a slow driver often leads delays to get angry and it can them late for work work'},\n",
       " {'name': 'system_34',\n",
       "  'content': 'Label: Entailment. Explanation: ItGetting behind a slow driver is leads delays to get angry and it can them late for work work'},\n",
       " {'name': 'system_35',\n",
       "  'content': 'Label: Entailment. Explanation: ItGetting behind a slow driver is leads  to be angry and it can them late for work work and'},\n",
       " {'name': 'system_4',\n",
       "  'content': 'Label: Entailment. Explanation: It is not a good thing to follow a slow driver because it makes someone late for work because it is a sign of frustration and frustration'},\n",
       " {'name': 'system_5',\n",
       "  'content': 'Label: Entailment. Explanation: It is often frustrating when someone follows the slow driver and it makes them late for work because it can lead to delays in the work and so it is natural to feel angry.'},\n",
       " {'name': 'system_6',\n",
       "  'content': 'Label: Entailment. Explanation: ItGetting behind a slow driver is leads frustration to get angry and they can them late for work work'},\n",
       " {'name': 'system_7',\n",
       "  'content': '. Explanation: ItGetting behind a slow driver is leads delays to get angry and it can them late for work work andabel: Entailment'},\n",
       " {'name': 'system_1',\n",
       "  'content': 'Label: Entailment. Explanation: T paper boy  to   and it broke the window and'},\n",
       " {'name': 'system_2',\n",
       "  'content': 'Label: Entailment. Explanation: The paper boy hard to  hard and it broke the window so'},\n",
       " {'name': 'system_31',\n",
       "  'content': 'Label: Entailment. Explanation: T paper boy  hard  hard and it broke the window and'},\n",
       " {'name': 'system_32',\n",
       "  'content': 'Label: Entailment. Explanation: The paper boy  to so  and it broke the window and. Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label Label'},\n",
       " {'name': 'system_33',\n",
       "  'content': 'Label: Entailment. Explanation: T paper boy  to   and it broke my window and'},\n",
       " {'name': 'system_34',\n",
       "  'content': 'Label: Entailment. Explanation: The paper boy  to so hard and it broke the window and'},\n",
       " {'name': 'system_35',\n",
       "  'content': 'Label: Entailment. Explanation: T paper was  to  hard and it broke my window and'},\n",
       " {'name': 'system_4',\n",
       "  'content': 'Label: Entailment. Explanation: Tripping the newspaper hard is a very embarrassing and unhygienic act and so the paper boy is flung the newspaper and breaking the window.'},\n",
       " {'name': 'system_5',\n",
       "  'content': 'Label: Entailment. Explanation: Tripping the newspaper is a very hard thing to do and so the paper boy is not doing that.'},\n",
       " {'name': 'system_6',\n",
       "  'content': 'Label: Entailment. Explanation: The paper boy so to great  and it broke the window in'},\n",
       " {'name': 'system_7',\n",
       "  'content': '. Explanation: T newspaper boy  very  hard and it broke the window andabel: Entailment'},\n",
       " {'name': 'system_1',\n",
       "  'content': 'Label: Entailment. Explanation: Tinted glass is  transparent, it is d and the fact.'},\n",
       " {'name': 'system_2',\n",
       "  'content': 'Label: Entailment. Explanation: Tinted glass is not transparent, it is d and the glass.'},\n",
       " {'name': 'system_31',\n",
       "  'content': 'Label: Entailment. Explanation: Tinted glass is not transparent, it is d or the glass.'},\n",
       " {'name': 'system_32',\n",
       "  'content': 'Label: Contratailment. Explanation: Tinted glass is  transparent, it is notd and the glass.'},\n",
       " {'name': 'system_33',\n",
       "  'content': 'Label: Entailment. Explanation: Tinted glass is not transparent, it is obscured and the operation of'},\n",
       " {'name': 'system_34',\n",
       "  'content': 'Label: Entailment. Explanation: tintted glass is not transparent, it is d and the glass.'},\n",
       " {'name': 'system_35',\n",
       "  'content': 'Label: Contratailment. Explanation: Tinted glass is  transparent, it is notd and the glass.'},\n",
       " {'name': 'system_4',\n",
       "  'content': 'Label: Entailment. Explanation: tinted glass is not transparent, so the simile is saying that the operation is not transparent.'},\n",
       " {'name': 'system_5',\n",
       "  'content': 'Label: Contradiction. Explanation: A tinted glass is a thin, transparent object, so saying something is as transparent as tinted glass would imply that it is opaque.'},\n",
       " {'name': 'system_6',\n",
       "  'content': 'Label: Entailment. Explanation: tintted glass is not transparent, it is notd and the sunlight of'},\n",
       " {'name': 'system_7',\n",
       "  'content': 'Label: Entailment. Explanation: Tinted glass is not transparent, it is notd and the factL'},\n",
       " {'name': 'ground_truth',\n",
       "  'content': 'Label: Entailment. Explanation: Tinted glass is not transparent because it is obscured by the color.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.append({\"name\": \"ground_truth\", \"content\": decoded_labels[0]})\n",
    "records[-8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n{'system_1': [0.5056115530372459, 0.5085812472965738], 'system_2': [0.5323239453833644, 0.5374525863206762], 'system_31': [0.5084813364883226, 0.5078069259932709], 'system_32': [0.5055255195638098, 0.5074808302929493], 'system_33': [0.5032615710245999, 0.508591734512484], 'system_34': [0.5067396302748032, 0.5087587260751042], 'system_35': [0.5036829298423671, 0.5073656197676405]}\\n{'system_1': [0.47329270980754923], 'system_2': [0.5039333868999851], 'system_31': [0.47420432731446144], 'system_32': [0.473849787491779], 'system_33': [0.47287909150934293], 'system_34': [0.47171914165316353], 'system_35': [0.47436950304876985]}\\n{'system_1': [0.5221458462682274, 0.4906728974226775], 'system_2': [0.5453307495230577, 0.5189864534869755], 'system_31': [0.524029905213074, 0.49432447432649923], 'system_32': [0.5236538569098267, 0.4949640794043393], 'system_33': [0.522696825432352, 0.49418902866839187], 'system_34': [0.5231002650281118, 0.49245360907178504], 'system_35': [0.5238553839735753, 0.4967562420314123]}\\n{'system_1': [0.5201383622010409, 0.5019083214066056], 'system_2': [0.5451744732719431, 0.5273190341222926], 'system_31': [0.5192974336052382, 0.5002995114825706], 'system_32': [0.5188841013023715, 0.5007719063422055], 'system_33': [0.5203219622545853, 0.49992736417041994], 'system_34': [0.5180675680247785, 0.5008015168679895], 'system_35': [0.5224425242712794, 0.5010754435268072]}\\n{'system_1': [0.49590791112913984], 'system_2': [0.5147096144113823], 'system_31': [0.4927430972029428], 'system_32': [0.49258799343269694], 'system_33': [0.49049952341283654], 'system_34': [0.49317378483047924], 'system_35': [0.4919880906784929]}\\n{'system_1': [0.498351050356349, 0.5090844074007058], 'system_2': [0.5289706096196247, 0.535423121872322], 'system_31': [0.5004418699772601, 0.5081924349139884], 'system_32': [0.5000023839289528, 0.5069172466751917], 'system_33': [0.49609349538092806, 0.5060478473679386], 'system_34': [0.49683869684920356, 0.5072891134960673], 'system_35': [0.49873229852851614, 0.5063988604083957]}\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "{'system_1': [0.5056115530372459, 0.5085812472965738], 'system_2': [0.5323239453833644, 0.5374525863206762], 'system_31': [0.5084813364883226, 0.5078069259932709], 'system_32': [0.5055255195638098, 0.5074808302929493], 'system_33': [0.5032615710245999, 0.508591734512484], 'system_34': [0.5067396302748032, 0.5087587260751042], 'system_35': [0.5036829298423671, 0.5073656197676405]}\n",
    "{'system_1': [0.47329270980754923], 'system_2': [0.5039333868999851], 'system_31': [0.47420432731446144], 'system_32': [0.473849787491779], 'system_33': [0.47287909150934293], 'system_34': [0.47171914165316353], 'system_35': [0.47436950304876985]}\n",
    "{'system_1': [0.5221458462682274, 0.4906728974226775], 'system_2': [0.5453307495230577, 0.5189864534869755], 'system_31': [0.524029905213074, 0.49432447432649923], 'system_32': [0.5236538569098267, 0.4949640794043393], 'system_33': [0.522696825432352, 0.49418902866839187], 'system_34': [0.5231002650281118, 0.49245360907178504], 'system_35': [0.5238553839735753, 0.4967562420314123]}\n",
    "{'system_1': [0.5201383622010409, 0.5019083214066056], 'system_2': [0.5451744732719431, 0.5273190341222926], 'system_31': [0.5192974336052382, 0.5002995114825706], 'system_32': [0.5188841013023715, 0.5007719063422055], 'system_33': [0.5203219622545853, 0.49992736417041994], 'system_34': [0.5180675680247785, 0.5008015168679895], 'system_35': [0.5224425242712794, 0.5010754435268072]}\n",
    "{'system_1': [0.49590791112913984], 'system_2': [0.5147096144113823], 'system_31': [0.4927430972029428], 'system_32': [0.49258799343269694], 'system_33': [0.49049952341283654], 'system_34': [0.49317378483047924], 'system_35': [0.4919880906784929]}\n",
    "{'system_1': [0.498351050356349, 0.5090844074007058], 'system_2': [0.5289706096196247, 0.535423121872322], 'system_31': [0.5004418699772601, 0.5081924349139884], 'system_32': [0.5000023839289528, 0.5069172466751917], 'system_33': [0.49609349538092806, 0.5060478473679386], 'system_34': [0.49683869684920356, 0.5072891134960673], 'system_35': [0.49873229852851614, 0.5063988604083957]}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This operation is obscured and not transparent. ',\n",
       " 'This operation is as transparent as tinted glass. Is there a contradiction or entailment between the premise and hypothesis?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system_1': [0.6544642734433673, 0.6538472479091643, 0.6451995135621926],\n",
       " 'system_2': [0.6591784410473625, 0.6556348348860563, 0.6485904916673478],\n",
       " 'system_31': [0.6616151178011304, 0.6529529681168658, 0.6457822724848103],\n",
       " 'system_32': [0.6602817794390899, 0.6454009215237158, 0.6454316035156613],\n",
       " 'system_33': [0.6577626817560993, 0.654545940696381, 0.6495673748102144],\n",
       " 'system_34': [0.6577570152042305, 0.6535457188788231, 0.6434608160222891],\n",
       " 'system_35': [0.6555744419703384, 0.6538046517382146, 0.6470891971959015],\n",
       " 'system_4': [0.52284036210826, 0.5163325460813661, 0.5156399803184772],\n",
       " 'system_5': [0.5240891672641392, 0.5080151274468501, 0.5057846437218605],\n",
       " 'system_6': [0.6635674632696552, 0.6627904866947751, 0.6585561479873518],\n",
       " 'system_7': [0.6611524853455227, 0.6590132359931218, 0.6561421023328687]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system_2 TtestResult(statistic=23.19102411438574, pvalue=1.2259514290288666e-09, df=9)\n",
      "system_31 TtestResult(statistic=0.5948201499859518, pvalue=0.2833078171850142, df=9)\n",
      "system_32 TtestResult(statistic=-0.15212858962856382, pvalue=0.5587793758146149, df=9)\n",
      "system_33 TtestResult(statistic=-1.4525349538900345, pvalue=0.9098456440410233, df=9)\n",
      "system_34 TtestResult(statistic=-1.3719083987600567, pvalue=0.898341997218993, df=9)\n",
      "system_35 TtestResult(statistic=0.10660074133722681, pvalue=0.45872204141041606, df=9)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# paired t test\n",
    "for mode in modes:\n",
    "    print(mode)\n",
    "    for key in list(modes[mode].keys())[1:]:\n",
    "        print(\"---\", key, ttest_rel(modes[mode][key], modes[mode][\"system_1\"], alternative=\"greater\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
