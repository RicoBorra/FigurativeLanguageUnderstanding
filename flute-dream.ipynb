{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yoan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, optimization\n",
    "import os \n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from copy import deepcopy\n",
    "from torch.optim import AdamW\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dream_checkpoint = \"RicoBorra/DREAM-t5-small\"\n",
    "\n",
    "dream_tokenizer = AutoTokenizer.from_pretrained(dream_checkpoint)\n",
    "dream_model = AutoModelForSeq2SeqLM.from_pretrained(dream_checkpoint)\n",
    "dream_data_collator = DataCollatorForSeq2Seq(tokenizer=dream_tokenizer, model=dream_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLUTE Data extraction and processing\n",
    "Using the few instructions from the Git Readme to have the same initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the train split is available on HuggingFace\n",
    "dataset = load_dataset(\"ColumbiaNLP/FLUTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dream_elaborations(dataset, dream_model, path_to_save) :\n",
    "    if os.path.exists(path_to_save) :\n",
    "        elaborations = pd.read_csv(path_to_save)\n",
    "    else :\n",
    "        elaborations = pd.DataFrame(columns=['premise_emotion', 'premise_motivation', 'premise_consequence', 'premise_rot',\n",
    "                                            'hypothesis_emotion', 'hypothesis_motivation', 'hypothesis_consequence', 'hypothesis_rot'])\n",
    "        for sentence_type in ['premise', 'hypothesis'] :\n",
    "            for dream_dimension in ['emotion', 'motivation', 'consequence', 'rot'] :\n",
    "                inputs = ['[SITUATION] ' + sentence + ' [QUERY] ' + dream_dimension for sentence in dataset[sentence_type]]\n",
    "                tokens = torch.tensor(tokenizer(inputs, padding='longest').input_ids)\n",
    "                output_tokens = dream_model.generate(tokens, max_new_tokens=100)\n",
    "                elaborations[sentence_type + '_' + dream_dimension] = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        # Make sure each sentence ends with a point\n",
    "        elaborations = elaborations.applymap(lambda x : x + '.' if not x.endswith('.') else x)\n",
    "        \n",
    "        elaborations.to_csv(path_to_save)\n",
    "\n",
    "    return elaborations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dreams = compute_dream_elaborations(dataset['train'], dream_model, \"dream_elaborations.csv\")\n",
    "\n",
    "def add_combined_cols(entry):\n",
    "    \n",
    "    premise = entry[\"premise\"].strip()\n",
    "    hypothesis = entry[\"hypothesis\"].strip()\n",
    "    \n",
    "    if not premise.endswith(\".\"):\n",
    "        premise += \".\"\n",
    "    assert(premise.endswith(\".\"))\n",
    "    if not hypothesis.endswith(\".\"):\n",
    "        hypothesis += \".\"\n",
    "    assert(hypothesis.endswith(\".\"))\n",
    "    \n",
    "    # Columns for System 1\n",
    "    entry[\"premise_hypothesis\"] = 'Premise: ' + premise + ' Hypothesis: ' + hypothesis + 'Is there a contradiction or entailment between the premise and hypothesis ?'\n",
    "    entry[\"label_explanation\"] = 'Label: ' + entry[\"label\"] + '. Explanation: ' + entry[\"explanation\"]\n",
    "\n",
    "    # Columns for System 2\n",
    "    entry[\"premise_hypothesis_system_2\"] = 'Premise: ' + premise + ' Hypothesis: ' + hypothesis + 'What is the type of figurative language involved? Is there a contradiction or entailment between the premise and hypothesis ?'\n",
    "    entry[\"type_label_explanation\"] = 'Type: ' + entry[\"type\"] + '. Label: ' + entry[\"label\"] + '. Explanation: ' + entry[\"explanation\"]\n",
    "    \n",
    "    # Columns for Systems 3\n",
    "    for dream_dimension in ['emotion', 'motivation', 'consequence', 'rot'] :\n",
    "        entry[\"premise_hypothesis_\" + dream_dimension] = 'Premise: ' + premise + '[' + dream_dimension.capitalize() + ']' + dreams['premise_' + dream_dimension].strip() + \\\n",
    "                    ' Hypothesis: ' + hypothesis + '[' + dream_dimension.capitalize() + ']' + dreams['hypothesis_' + dream_dimension] + 'Is there a contradiction or entailment between the premise and hypothesis ?'\n",
    "    entry[\"premise_hypothesis_all_dims\"] = 'Premise: ' + premise + \\\n",
    "                '[Emotion]' + dreams['premise_emotion'].strip() + \\\n",
    "                '[Motivation]' + dreams['premise_motivation'].strip() + \\\n",
    "                '[Consequence]' + dreams['premise_consequence'].strip() + \\\n",
    "                '[Rot]' + dreams['premise_rot'].strip() + \\\n",
    "                ' Hypothesis: ' + hypothesis + \\\n",
    "                '[Emotion]' + dreams['hypothesis_emotion'].strip() + \\\n",
    "                '[Motivation]' + dreams['hypothesis_motivation'].strip() + \\\n",
    "                '[Consequence]' + dreams['hypothesis_consequence'].strip() + \\\n",
    "                '[Rot]' + dreams['hypothesis_rot'].strip()\n",
    "    return entry\n",
    "# combine columns\n",
    "combined_cols_dataset = dataset['train'].map(add_combined_cols)\n",
    "\n",
    "# create train test split because given data has only train data\n",
    "# splits are shuffled by default\n",
    "dataset_train_test = combined_cols_dataset.train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predicted_token_ids = torch.argmax(torch.from_numpy(predictions[0]), dim=-1)\n",
    "    decoded_preds = tokenizer.batch_decode(predicted_token_ids, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System 1 : Normal classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s1 = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(examples):\n",
    "    model_inputs = tokenizer(examples['premise_hypothesis'])\n",
    "    labels = tokenizer(examples['label_explanation'])\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds = dataset_train_test.map(preprocess_dataset, batched=True)\n",
    "tokenized_ds = tokenized_ds.remove_columns(dataset_train_test['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24112 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  2%|▏         | 501/24112 [01:00<44:48,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3937, 'learning_rate': 4.896317186463172e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1001/24112 [02:02<47:53,  8.04it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0177, 'learning_rate': 4.792634372926344e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1501/24112 [03:07<41:30,  9.08it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9162, 'learning_rate': 4.688951559389516e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2001/24112 [04:14<45:05,  8.17it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8441, 'learning_rate': 4.585268745852688e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2501/24112 [05:17<47:17,  7.62it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8173, 'learning_rate': 4.4815859323158594e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3001/24112 [06:22<40:52,  8.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8065, 'learning_rate': 4.3779031187790315e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 12%|█▎        | 3014/24112 [06:48<41:56,  8.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5989887714385986, 'eval_runtime': 24.1193, 'eval_samples_per_second': 62.481, 'eval_steps_per_second': 31.261, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 3501/24112 [07:47<43:26,  7.91it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7179, 'learning_rate': 4.274220305242203e-05, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 4001/24112 [08:56<44:10,  7.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7005, 'learning_rate': 4.170537491705375e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 4501/24112 [09:53<36:40,  8.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7, 'learning_rate': 4.066854678168547e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 5001/24112 [10:55<36:56,  8.62it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.667, 'learning_rate': 3.963171864631719e-05, 'epoch': 1.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 5501/24112 [12:02<39:06,  7.93it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6764, 'learning_rate': 3.8594890510948907e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 6001/24112 [13:09<35:09,  8.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6123, 'learning_rate': 3.755806237558063e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 25%|██▌       | 6028/24112 [13:46<37:19,  8.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5124155282974243, 'eval_runtime': 33.65, 'eval_samples_per_second': 44.785, 'eval_steps_per_second': 22.407, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 6501/24112 [14:58<43:05,  6.81it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.626, 'learning_rate': 3.652123424021234e-05, 'epoch': 2.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 7001/24112 [16:04<36:23,  7.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.567, 'learning_rate': 3.548440610484406e-05, 'epoch': 2.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 7501/24112 [17:13<38:22,  7.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5735, 'learning_rate': 3.4447577969475784e-05, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 8001/24112 [18:18<34:50,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5688, 'learning_rate': 3.3410749834107505e-05, 'epoch': 2.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 8501/24112 [19:25<34:31,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5799, 'learning_rate': 3.237392169873922e-05, 'epoch': 2.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 9001/24112 [20:40<41:49,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5713, 'learning_rate': 3.133709356337094e-05, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 38%|███▊      | 9042/24112 [21:23<40:45,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4648537635803223, 'eval_runtime': 36.7605, 'eval_samples_per_second': 40.995, 'eval_steps_per_second': 20.511, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 9501/24112 [22:39<43:07,  5.65it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5161, 'learning_rate': 3.0300265428002654e-05, 'epoch': 3.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 10001/24112 [23:57<28:06,  8.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5249, 'learning_rate': 2.9263437292634375e-05, 'epoch': 3.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 10501/24112 [24:58<31:17,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4821, 'learning_rate': 2.8226609157266093e-05, 'epoch': 3.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 11001/24112 [26:04<26:11,  8.34it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4917, 'learning_rate': 2.7189781021897807e-05, 'epoch': 3.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 11501/24112 [27:15<23:06,  9.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5155, 'learning_rate': 2.615295288652953e-05, 'epoch': 3.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 12002/24112 [28:15<22:04,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5253, 'learning_rate': 2.5116124751161246e-05, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 50%|█████     | 12056/24112 [28:45<30:56,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.434415578842163, 'eval_runtime': 24.5448, 'eval_samples_per_second': 61.398, 'eval_steps_per_second': 30.719, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 12501/24112 [29:37<20:18,  9.53it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4731, 'learning_rate': 2.4079296615792967e-05, 'epoch': 4.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 13001/24112 [30:40<20:35,  8.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4942, 'learning_rate': 2.3042468480424688e-05, 'epoch': 4.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 13501/24112 [31:45<19:47,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4639, 'learning_rate': 2.2005640345056406e-05, 'epoch': 4.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 14001/24112 [32:53<19:56,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4378, 'learning_rate': 2.0968812209688123e-05, 'epoch': 4.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 14501/24112 [33:55<17:37,  9.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4826, 'learning_rate': 1.9931984074319844e-05, 'epoch': 4.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 15001/24112 [34:56<16:53,  8.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4389, 'learning_rate': 1.8895155938951562e-05, 'epoch': 4.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 62%|██████▎   | 15070/24112 [35:28<20:49,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4145468473434448, 'eval_runtime': 24.2577, 'eval_samples_per_second': 62.125, 'eval_steps_per_second': 31.083, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 15501/24112 [36:20<17:59,  7.98it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4424, 'learning_rate': 1.785832780358328e-05, 'epoch': 5.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 16001/24112 [37:24<15:24,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3914, 'learning_rate': 1.6821499668214997e-05, 'epoch': 5.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 16501/24112 [38:26<18:07,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4264, 'learning_rate': 1.5784671532846715e-05, 'epoch': 5.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 17001/24112 [39:30<13:05,  9.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.448, 'learning_rate': 1.4747843397478434e-05, 'epoch': 5.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 17502/24112 [40:27<12:13,  9.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4369, 'learning_rate': 1.3711015262110152e-05, 'epoch': 5.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 18001/24112 [41:29<12:08,  8.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4389, 'learning_rate': 1.2674187126741871e-05, 'epoch': 5.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 75%|███████▌  | 18084/24112 [42:05<11:21,  8.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4025543928146362, 'eval_runtime': 25.9616, 'eval_samples_per_second': 58.047, 'eval_steps_per_second': 29.043, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 18501/24112 [42:56<10:46,  8.68it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3917, 'learning_rate': 1.163735899137359e-05, 'epoch': 6.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 19001/24112 [43:56<10:03,  8.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.413, 'learning_rate': 1.0600530856005308e-05, 'epoch': 6.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 19501/24112 [45:03<09:24,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.398, 'learning_rate': 9.563702720637027e-06, 'epoch': 6.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 20001/24112 [46:08<08:11,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3996, 'learning_rate': 8.526874585268747e-06, 'epoch': 6.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 20501/24112 [47:13<06:57,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4054, 'learning_rate': 7.490046449900464e-06, 'epoch': 6.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 21001/24112 [48:17<05:45,  9.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4127, 'learning_rate': 6.453218314532184e-06, 'epoch': 6.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 88%|████████▊ | 21098/24112 [49:00<05:53,  8.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3936963081359863, 'eval_runtime': 30.2886, 'eval_samples_per_second': 49.755, 'eval_steps_per_second': 24.894, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 21501/24112 [50:00<06:25,  6.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4169, 'learning_rate': 5.416390179163902e-06, 'epoch': 7.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 22001/24112 [51:14<04:59,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3759, 'learning_rate': 4.379562043795621e-06, 'epoch': 7.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 22501/24112 [52:28<04:37,  5.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3879, 'learning_rate': 3.3427339084273395e-06, 'epoch': 7.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 23001/24112 [53:41<03:20,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3855, 'learning_rate': 2.305905773059058e-06, 'epoch': 7.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 23501/24112 [54:52<01:32,  6.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3851, 'learning_rate': 1.2690776376907765e-06, 'epoch': 7.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 24001/24112 [55:56<00:16,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3895, 'learning_rate': 2.3224950232249503e-07, 'epoch': 7.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "100%|██████████| 24112/24112 [56:41<00:00,  7.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3940967321395874, 'eval_runtime': 30.05, 'eval_samples_per_second': 50.15, 'eval_steps_per_second': 25.092, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24112/24112 [56:43<00:00,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3403.4563, 'train_samples_per_second': 14.167, 'train_steps_per_second': 7.085, 'train_loss': 1.5525400064287393, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24112, training_loss=1.5525400064287393, metrics={'train_runtime': 3403.4563, 'train_samples_per_second': 14.167, 'train_steps_per_second': 7.085, 'train_loss': 1.5525400064287393, 'epoch': 8.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''The following parameters were taken from the DREAM-FLUTE paper (only the number of epochs has been increased because the model is smaller)'''\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"D:\\Documents\\PoliTo\\Deep NLP\\Project\\S1Model_more_accurate\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    seed=42,\n",
    "    #weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=8,\n",
    "    load_best_model_at_end=True,\n",
    "    #eval_accumulation_steps=8,\n",
    "    #fp16=True,\n",
    "    #push_to_hub=True,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-08,\n",
    "    lr_scheduler_type='linear'\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_s1,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> Label: Contradiction. Explanation: Accidentally driving a car is not a good thing and so someone feeling like a champion when they crash it is not a good thing.</s>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_s1 = AutoModelForSeq2SeqLM.from_pretrained(\"D:\\Documents\\PoliTo\\Deep NLP\\Project\\S1Model_more_accurate\\checkpoint-24112\")\n",
    "i = \"Premise: Today I crashed my car. Hypothesis: I felt like a champion when I crashed my car.\"\n",
    "t = tokenizer(i, return_tensors='pt').input_ids\n",
    "t = t.to(model_s1.device)\n",
    "o = model_s1.generate(t, max_new_tokens = 100)\n",
    "d = tokenizer.decode(o[0])\n",
    "d "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System 2 : Predict type of figurative language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
