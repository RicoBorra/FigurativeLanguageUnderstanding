{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "BASE_DIR = Path(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_snli_train_1 = pd.read_csv(BASE_DIR / \"datasets\" / \"esnli_train_1.csv\")\n",
    "e_snli_train_2 = pd.read_csv(BASE_DIR / \"datasets\" / \"esnli_train_2.csv\")\n",
    "e_snli_train = pd.concat([e_snli_train_1, e_snli_train_2])\n",
    "\n",
    "e_snli_test = pd.read_csv(BASE_DIR / \"datasets\" / \"esnli_test.csv\")\n",
    "\n",
    "e_snli_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning on E-SNLI\n",
    "\n",
    "From the FLUTE paper:\n",
    "\n",
    "> T5e-SNLI: e-SNLI (Camburu et al., 2018) dataset\n",
    "> comes with supervised ground-truth labels and ra-\n",
    "> tionales. We fine-tune the 3B version of T5 on\n",
    "> e-SNLI for one epoch with a batch size of 1024,\n",
    "> and an AdamW Optimizer with a learning rate of\n",
    "> 1e − 4. We remove the Neutral examples from\n",
    "> e-SNLI because our test data does not have such\n",
    "> a category. We take the longest explanation per\n",
    "> example in e-SNLI since our data has only one ref-\n",
    "> erence explanation. In case the explanations are\n",
    "> more than one sentence we join them using ‘and’\n",
    "> since our data contains single-sentence explana-\n",
    "> tions. This leaves us with 366,603 training and\n",
    "> 6,607 validation examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_snli_train = e_snli_train[e_snli_train[\"gold_label\"] != \"neutral\"]\n",
    "e_snli_test = e_snli_test[e_snli_test[\"gold_label\"] != \"neutral\"]\n",
    "\n",
    "\n",
    "def join_sentences(explanation):\n",
    "    return str(explanation).replace(\". \", \" and \")\n",
    "\n",
    "\n",
    "e_snli_train[\"Explanation_1\"] = e_snli_train[\"Explanation_1\"].apply(join_sentences)\n",
    "\n",
    "\n",
    "def find_longest_explanation(row):\n",
    "    explanations = [row[\"Explanation_1\"], row[\"Explanation_2\"], row[\"Explanation_3\"]]\n",
    "    return max(explanations, key=len)\n",
    "\n",
    "\n",
    "e_snli_test[\"Explanation\"] = e_snli_test.apply(find_longest_explanation, axis=1)\n",
    "e_snli_test[\"Explanation\"] = e_snli_test[\"Explanation\"].apply(join_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "train_dataset = Dataset.from_pandas(e_snli_train)\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_name)\n",
    "\n",
    "\n",
    "def preprocess_train(batch):\n",
    "    pairIDs = batch[\"pairID\"]\n",
    "    gold_labels = batch[\"gold_label\"]\n",
    "    Sentence1s = batch[\"Sentence1\"]\n",
    "    Sentence2s = batch[\"Sentence2\"]\n",
    "    explanation_1 = batch[\"Explanation_1\"]\n",
    "\n",
    "    inputs = [\n",
    "        f\"Does the sentence '{s1}' entail or contradict the sentence '{s2}'? Please answer between 'Entails' or 'Contradicts' and explain your decision in a sentence.\"\n",
    "        for s1, s2 in zip(Sentence1s, Sentence2s)\n",
    "    ]\n",
    "\n",
    "    targets = [\n",
    "        f\"{label} - {explanation}\"\n",
    "        for label, explanation in zip(gold_labels, explanation_1)\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        inputs,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        targets,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return {\"input_ids\": inputs[\"input_ids\"], \"labels\": targets[\"input_ids\"]}\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_train, batched=True)\n",
    "tokenized_train = tokenized_train.remove_columns(\n",
    "    [\n",
    "        \"pairID\",\n",
    "        \"gold_label\",\n",
    "        \"Sentence1\",\n",
    "        \"Sentence2\",\n",
    "        \"Explanation_1\",\n",
    "        \"WorkerId\",\n",
    "        \"Sentence1_marked_1\",\n",
    "        \"Sentence2_marked_1\",\n",
    "        \"Sentence1_Highlighted_1\",\n",
    "        \"Sentence2_Highlighted_1\",\n",
    "        \"__index_level_0__\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test(batch):\n",
    "    pairIDs = batch[\"pairID\"]\n",
    "    gold_labels = batch[\"gold_label\"]\n",
    "    Sentence1s = batch[\"Sentence1\"]\n",
    "    Sentence2s = batch[\"Sentence2\"]\n",
    "    explanation = batch[\"Explanation\"]\n",
    "\n",
    "    inputs = [\n",
    "        f\"Does the sentence '{s1}' entail or contradict the sentence '{s2}'? Please answer between 'Entails' or 'Contradicts' and explain your decision in a sentence.\"\n",
    "        for s1, s2 in zip(Sentence1s, Sentence2s)\n",
    "    ]\n",
    "\n",
    "    targets = [\n",
    "        f\"{label} - {explanation}\"\n",
    "        for label, explanation in zip(gold_labels, explanation)\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        inputs,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        targets,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return {\"input_ids\": inputs[\"input_ids\"], \"labels\": targets[\"input_ids\"]}\n",
    "\n",
    "\n",
    "test_dataset = Dataset.from_pandas(e_snli_test)\n",
    "tokenized_test = test_dataset.map(preprocess_test, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# TODO: Is this correct? Test.\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    print(\"Before decoding\")\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    print(decoded_preds)\n",
    "    print(decoded_labels)\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    print(\"result\", result)\n",
    "    return result[\"rouge1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_eval_batch_size=32,  # smaller batch size to run on less VRAM\n",
    "    per_device_train_batch_size=32,  # smaller batch size to run on less VRAM\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=1,\n",
    "    save_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "del tokenized_train\n",
    "del tokenized_test\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"models/checkpoint-11450\")\n",
    "\n",
    "P = \"I bet I am blue.\"\n",
    "H = \"I bet I am like a cherry.\"\n",
    "prompt = f\"Does the sentence '{P}' entail or contradict the sentence '{H}'? Please answer between 'Entails' or 'Contradicts' and explain your decision in a sentence.\"\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "tokens = tokens.to(model.device)\n",
    "output = model.generate(tokens, max_new_tokens=100)\n",
    "output = tokenizer.decode(output[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning on FLUTE\n",
    "\n",
    "From the FLUTE paper:\n",
    "\n",
    "> We fine-tune the 3B version of T5\n",
    "> model for 10 epochs with a batch size of 1024, and\n",
    "> an AdamW Optimizer with a learning rate of 1e−4\n",
    "> in a multitask fashion with data from all the four\n",
    "> types of figurative languages combined. Our train-\n",
    "> ing data consists of 7,035 samples which is 50X\n",
    "> smaller than e-SNLI. For validation we use 500 ex-\n",
    "> amples which is used for selecting best checkpoint\n",
    "> based on loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flute_dataset = pd.read_json(BASE_DIR / \"datasets\" / \"train.jsonl\", lines=True)\n",
    "\n",
    "print(len(flute_dataset))\n",
    "flute_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "flute_dataset = Dataset.from_pandas(flute_dataset)\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "flute_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_name)\n",
    "\n",
    "\n",
    "def preprocess_train(batch):\n",
    "    premises = batch[\"premise\"]\n",
    "    hypothesis = batch[\"hypothesis\"]\n",
    "    labels = batch[\"label\"]\n",
    "    explanations = batch[\"explanation\"]\n",
    "\n",
    "    inputs = [\n",
    "        f\"Does the sentence '{s1}' entail or contradict the sentence '{s2}'? Please answer between 'Entails' or 'Contradicts' and explain your decision in a sentence.\"\n",
    "        for s1, s2 in zip(hypothesis, premises)\n",
    "    ]\n",
    "\n",
    "    targets = [\n",
    "        f\"{label} - {explanation}\" for label, explanation in zip(labels, explanations)\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        inputs,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        targets,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return {\"input_ids\": inputs[\"input_ids\"], \"labels\": targets[\"input_ids\"]}\n",
    "\n",
    "\n",
    "tokenized_dataset = flute_dataset.map(preprocess_train, batched=True)\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(\n",
    "    [\"id\", \"premise\", \"hypothesis\", \"label\", \"explanation\", \"split\", \"type\"]\n",
    ")\n",
    "\n",
    "train_dataset = tokenized_dataset.shuffle(seed=42).select(range(7035))\n",
    "test_dataset = tokenized_dataset.select(range(7035, len(tokenized_dataset)))\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_eval_batch_size=32,\n",
    "    per_device_train_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=1,\n",
    "    save_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "del train_dataset\n",
    "del test_dataset\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"models/checkpoint-2200\")\n",
    "\n",
    "P = \"I bet I am blue.\"\n",
    "H = \"I bet I am like a cherry.\"\n",
    "prompt = f\"Does the sentence '{P}' entail or contradict the sentence '{H}'? Please answer between 'Entails' or 'Contradicts' and explain your decision in a sentence.\"\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "tokens = tokens.to(model.device)\n",
    "output = model.generate(tokens, max_new_tokens=100)\n",
    "output = tokenizer.decode(output[0])\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
