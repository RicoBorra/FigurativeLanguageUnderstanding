{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, optimization, Pipeline\n",
    "import os \n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from copy import deepcopy\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dream_checkpoint = \"RicoBorra/DREAM-t5-small\"\n",
    "\n",
    "dream_tokenizer = AutoTokenizer.from_pretrained(dream_checkpoint)\n",
    "dream_model = AutoModelForSeq2SeqLM.from_pretrained(dream_checkpoint)\n",
    "dream_data_collator = DataCollatorForSeq2Seq(tokenizer=dream_tokenizer, model=dream_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLUTE Data extraction and processing\n",
    "Using the few instructions from the Git Readme to have the same initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the train split is available on HuggingFace\n",
    "dataset = load_dataset(\"ColumbiaNLP/FLUTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dream_elaborations(dataset, dream_model, path_to_save, batch_size=32):\n",
    "    if os.path.exists(path_to_save):\n",
    "        elaborations = pd.read_csv(path_to_save)\n",
    "    else:\n",
    "        elaborations = pd.DataFrame(columns=['premise_emotion', 'premise_motivation', 'premise_consequence', 'premise_rot',\n",
    "                                            'hypothesis_emotion', 'hypothesis_motivation', 'hypothesis_consequence', 'hypothesis_rot'])\n",
    "        for sentence_type in tqdm(['premise', 'hypothesis'], desc=\"Processing Sentence Type\"):\n",
    "            for dream_dimension in tqdm(['emotion', 'motivation', 'consequence', 'rot'], desc=\"Processing Dream Dimension\"):\n",
    "                # Split the dataset into batches to prevent the hard disk to be filled\n",
    "                inputs_batches = [dataset[sentence_type][i:i+batch_size] for i in range(0, len(dataset), batch_size)]\n",
    "                output_sentences = []\n",
    "\n",
    "                # Process each batch\n",
    "                for inputs_batch in tqdm(inputs_batches, desc=\"Processing Batches\", leave=False):\n",
    "                    inputs = ['[SITUATION] ' + sentence + ' [QUERY] ' + dream_dimension for sentence in inputs_batch]\n",
    "                    tokens = torch.tensor(tokenizer(inputs, padding='longest').input_ids)\n",
    "                    output_tokens = dream_model.generate(tokens, max_new_tokens=100)\n",
    "                    output_sentences.extend(tokenizer.batch_decode(output_tokens, skip_special_tokens=True))\n",
    "\n",
    "                elaborations[sentence_type + '_' + dream_dimension] = output_sentences\n",
    "\n",
    "        # Make sure each sentence ends with a point\n",
    "        elaborations = elaborations.applymap(lambda x: x + '.' if not x.endswith('.') else x)\n",
    "\n",
    "        elaborations.to_csv(path_to_save)\n",
    "\n",
    "    return elaborations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dreams = compute_dream_elaborations(dataset['train'], dream_model, \"dream_elaborations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset = pd.concat([pd.DataFrame(dataset['train']), dreams], axis=1)\n",
    "complete_dataset = Dataset.from_pandas(complete_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset.to_csv(\"complete_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_combined_cols(entry):\n",
    "    \n",
    "    premise = entry[\"premise\"].strip()\n",
    "    hypothesis = entry[\"hypothesis\"].strip()\n",
    "    \n",
    "    if not premise.endswith(\".\"):\n",
    "        premise += \".\"\n",
    "    assert(premise.endswith(\".\"))\n",
    "    if not hypothesis.endswith(\".\"):\n",
    "        hypothesis += \".\"\n",
    "    assert(hypothesis.endswith(\".\"))\n",
    "    \n",
    "    # Columns for System 1\n",
    "    entry[\"premise_hypothesis\"] = 'Premise: ' + premise + ' Hypothesis: ' + hypothesis + ' Is there a contradiction or entailment between the premise and hypothesis ?'\n",
    "    entry[\"label_explanation\"] = 'Label: ' + entry[\"label\"] + '. Explanation: ' + entry[\"explanation\"]\n",
    "\n",
    "    # Columns for System 2\n",
    "    entry[\"premise_hypothesis_system_2\"] = 'Premise: ' + premise + ' Hypothesis: ' + hypothesis + ' What is the type of figurative language involved? Is there a contradiction or entailment between the premise and hypothesis ?'\n",
    "    entry[\"type_label_explanation\"] = 'Type: ' + entry[\"type\"] + '. Label: ' + entry[\"label\"] + '. Explanation: ' + entry[\"explanation\"]\n",
    "    \n",
    "    # Columns for Systems 3\n",
    "    for dream_dimension in ['emotion', 'motivation', 'consequence', 'rot'] :\n",
    "        entry[\"premise_hypothesis_\" + dream_dimension] = 'Premise: ' + premise + ' [' + dream_dimension.capitalize() + '] ' + entry['premise_' + dream_dimension].strip() + \\\n",
    "                    ' Hypothesis: ' + hypothesis + ' [' + dream_dimension.capitalize() + '] ' + entry['hypothesis_' + dream_dimension] + ' Is there a contradiction or entailment between the premise and hypothesis ?'\n",
    "    entry[\"premise_hypothesis_all_dims\"] = 'Premise: ' + premise + \\\n",
    "                ' [Emotion] ' + entry['premise_emotion'].strip() + \\\n",
    "                ' [Motivation] ' + entry['premise_motivation'].strip() + \\\n",
    "                ' [Consequence] ' + entry['premise_consequence'].strip() + \\\n",
    "                ' [Rot] ' + entry['premise_rot'].strip() + \\\n",
    "                ' Hypothesis: ' + hypothesis + \\\n",
    "                ' [Emotion] ' + entry['hypothesis_emotion'].strip() + \\\n",
    "                ' [Motivation] ' + entry['hypothesis_motivation'].strip() + \\\n",
    "                ' [Consequence] ' + entry['hypothesis_consequence'].strip() + \\\n",
    "                ' [Rot] ' + entry['hypothesis_rot'].strip()\n",
    "    \n",
    "    # Columns for System 4 (For the explanation part)\n",
    "    '''As the specific input isn't indicated in the paper, the question tries to formalize at best what is expected'''\n",
    "    entry[\"premise_hypothesis_label\"] = 'Premise: ' + premise + ' Hypothesis: ' + hypothesis + ' Label : ' + entry['label'] + '. What is the explanation of the label associated to the premise and the hypothesis ?'\n",
    "    \n",
    "    return entry\n",
    "# combine columns\n",
    "combined_cols_dataset = complete_dataset.map(add_combined_cols)\n",
    "\n",
    "# create train test split because given data has only train data\n",
    "# splits are shuffled by default\n",
    "dataset_train_test = combined_cols_dataset.train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predicted_token_ids = torch.argmax(torch.from_numpy(predictions[0]), dim=-1)\n",
    "    decoded_preds = tokenizer.batch_decode(predicted_token_ids, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System 1 : Normal classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s1 = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_s1(examples):\n",
    "    model_inputs = tokenizer(examples['premise_hypothesis'])\n",
    "    labels = tokenizer(examples['label_explanation'])\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds_s1 = dataset_train_test.map(preprocess_dataset_s1, batched=True).remove_columns(dataset_train_test['train'].column_names)\n",
    "tokenized_ds_s1 = tokenized_ds_s1.remove_columns(dataset_train_test['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The following parameters were taken from the DREAM-FLUTE paper (only the number of epochs has been increased because the model is smaller)'''\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"D:\\Documents\\PoliTo\\Deep NLP\\Project\\S1Model_more_accurate\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    seed=42,\n",
    "    #weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=8,\n",
    "    load_best_model_at_end=True,\n",
    "    #eval_accumulation_steps=8,\n",
    "    #fp16=True,\n",
    "    #push_to_hub=True,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-08,\n",
    "    lr_scheduler_type='linear'\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_s1,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds_s1[\"train\"],\n",
    "    eval_dataset=tokenized_ds_s1[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "# Delete variables after training to prevent memory overflow for the next trainings\n",
    "del tokenized_ds_s1\n",
    "del model_s1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s1 = AutoModelForSeq2SeqLM.from_pretrained(\"D:\\Documents\\PoliTo\\Deep NLP\\Project\\S1Model_more_accurate\\checkpoint-24112\")\n",
    "i = \"Premise: Today I crashed my car. Hypothesis: I felt like a champion when I crashed my car.\"\n",
    "t = tokenizer(i, return_tensors='pt').input_ids\n",
    "t = t.to(model_s1.device)\n",
    "o = model_s1.generate(t, max_new_tokens = 100)\n",
    "d = tokenizer.decode(o[0])\n",
    "d "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System 2 : Predict type of figurative language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s2 = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_s2(examples):\n",
    "    model_inputs = tokenizer(examples['premise_hypothesis_system_2'])\n",
    "    labels = tokenizer(examples['type_label_explanation'])\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds_s2 = dataset_train_test.map(preprocess_dataset_s2, batched=True)\n",
    "tokenized_ds_s2 = tokenized_ds_s2.remove_columns(dataset_train_test['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The following parameters were taken from the DREAM-FLUTE paper (only the number of epochs has been increased because the model is smaller)'''\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"D:\\Documents\\PoliTo\\Deep NLP\\Project\\S2Model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    seed=42,\n",
    "    #weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=8,\n",
    "    load_best_model_at_end=True,\n",
    "    #eval_accumulation_steps=8,\n",
    "    #fp16=True,\n",
    "    #push_to_hub=True,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-08,\n",
    "    lr_scheduler_type='linear'\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_s2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds_s2[\"train\"],\n",
    "    eval_dataset=tokenized_ds_s2[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "del tokenized_ds_s2\n",
    "del model_s2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System 3 : Include DREAM elaborations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 : Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s31 = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_s31(examples):\n",
    "    model_inputs = tokenizer(examples['premise_hypothesis_emotion'])\n",
    "    labels = tokenizer(examples['label_explanation'])\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds_s31 = dataset_train_test.map(preprocess_dataset_s31, batched=True)\n",
    "tokenized_ds_s31 = tokenized_ds_s31.remove_columns(dataset_train_test['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The following parameters were taken from the DREAM-FLUTE paper (only the number of epochs has been increased because the model is smaller)'''\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"D:\\Documents\\PoliTo\\Deep NLP\\Project\\S3-1Model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    seed=42,\n",
    "    #weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=8,\n",
    "    load_best_model_at_end=True,\n",
    "    #eval_accumulation_steps=8,\n",
    "    #fp16=True,\n",
    "    #push_to_hub=True,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-08,\n",
    "    lr_scheduler_type='linear'\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_s31,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds_s31[\"train\"],\n",
    "    eval_dataset=tokenized_ds_s31[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "del tokenized_ds_s31\n",
    "del model_s31\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 : Motivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s32 = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_s32(examples):\n",
    "    model_inputs = tokenizer(examples['premise_hypothesis_motivation'])\n",
    "    labels = tokenizer(examples['label_explanation'])\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds_s32 = dataset_train_test.map(preprocess_dataset_s32, batched=True)\n",
    "tokenized_ds_s32 = tokenized_ds_s32.remove_columns(dataset_train_test['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The following parameters were taken from the DREAM-FLUTE paper (only the number of epochs has been increased because the model is smaller)'''\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"D:\\Documents\\PoliTo\\Deep NLP\\Project\\S3-2Model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    seed=42,\n",
    "    #weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=8,\n",
    "    load_best_model_at_end=True,\n",
    "    #eval_accumulation_steps=8,\n",
    "    #fp16=True,\n",
    "    #push_to_hub=True,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-08,\n",
    "    lr_scheduler_type='linear'\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_s32,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds_s32[\"train\"],\n",
    "    eval_dataset=tokenized_ds_s32[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "del tokenized_ds_s32\n",
    "del model_s32\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 : Consequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s33 = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_s33(examples):\n",
    "    model_inputs = tokenizer(examples['premise_hypothesis_consequence'])\n",
    "    labels = tokenizer(examples['label_explanation'])\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds_s33 = dataset_train_test.map(preprocess_dataset_s33, batched=True)\n",
    "tokenized_ds_s33 = tokenized_ds_s33.remove_columns(dataset_train_test['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The following parameters were taken from the DREAM-FLUTE paper (only the number of epochs has been increased because the model is smaller)'''\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"D:\\Documents\\PoliTo\\Deep NLP\\Project\\S3-3Model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    seed=42,\n",
    "    #weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=8,\n",
    "    load_best_model_at_end=True,\n",
    "    #eval_accumulation_steps=8,\n",
    "    #fp16=True,\n",
    "    #push_to_hub=True,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-08,\n",
    "    lr_scheduler_type='linear'\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_s33,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds_s33[\"train\"],\n",
    "    eval_dataset=tokenized_ds_s33[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "del tokenized_ds_s33\n",
    "del model_s33\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 : Rule of Thumb (Social norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s34 = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_s34(examples):\n",
    "    model_inputs = tokenizer(examples['premise_hypothesis_rot'])\n",
    "    labels = tokenizer(examples['label_explanation'])\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds_s34 = dataset_train_test.map(preprocess_dataset_s34, batched=True)\n",
    "tokenized_ds_s34 = tokenized_ds_s34.remove_columns(dataset_train_test['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The following parameters were taken from the DREAM-FLUTE paper (only the number of epochs has been increased because the model is smaller)'''\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"D:\\Documents\\PoliTo\\Deep NLP\\Project\\S3-4Model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    seed=42,\n",
    "    #weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=8,\n",
    "    load_best_model_at_end=True,\n",
    "    #eval_accumulation_steps=8,\n",
    "    #fp16=True,\n",
    "    #push_to_hub=True,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-08,\n",
    "    lr_scheduler_type='linear'\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_s34,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds_s34[\"train\"],\n",
    "    eval_dataset=tokenized_ds_s34[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "del tokenized_ds_s34\n",
    "del model_s34\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 : All Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s35 = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_s35(examples):\n",
    "    model_inputs = tokenizer(examples['premise_hypothesis_all_dims'])\n",
    "    labels = tokenizer(examples['label_explanation'])\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds_s35 = dataset_train_test.map(preprocess_dataset_s35, batched=True)\n",
    "tokenized_ds_s35 = tokenized_ds_s35.remove_columns(dataset_train_test['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The following parameters were taken from the DREAM-FLUTE paper (only the number of epochs has been increased because the model is smaller)'''\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"D:\\Documents\\PoliTo\\Deep NLP\\Project\\S3-5Model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    seed=42,\n",
    "    #weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=8,\n",
    "    load_best_model_at_end=True,\n",
    "    #eval_accumulation_steps=8,\n",
    "    #fp16=True,\n",
    "    #push_to_hub=True,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-08,\n",
    "    lr_scheduler_type='linear'\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_s35,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds_s35[\"train\"],\n",
    "    eval_dataset=tokenized_ds_s35[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "del tokenized_ds_s35\n",
    "del model_s35\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System 4 : Two-step Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 : Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s41 = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_s41(examples):\n",
    "    model_inputs = tokenizer(examples['premise_hypothesis'])\n",
    "    labels = tokenizer(examples['label'])\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds_s41 = dataset_train_test.map(preprocess_dataset_s41, batched=True)\n",
    "tokenized_ds_s41 = tokenized_ds_s41.remove_columns(dataset_train_test['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The following parameters were taken from the DREAM-FLUTE paper (only the number of epochs has been increased because the model is smaller)'''\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"D:\\Documents\\PoliTo\\Deep NLP\\Project\\S4-1Model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    seed=42,\n",
    "    #weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=8,\n",
    "    load_best_model_at_end=True,\n",
    "    #eval_accumulation_steps=8,\n",
    "    #fp16=True,\n",
    "    #push_to_hub=True,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-08,\n",
    "    lr_scheduler_type='linear'\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_s41,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds_s41[\"train\"],\n",
    "    eval_dataset=tokenized_ds_s41[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "del tokenized_ds_s41\n",
    "del model_s41\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 : Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s42 = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_s42(examples):\n",
    "    model_inputs = tokenizer(examples['premise_hypothesis_label'])\n",
    "    labels = tokenizer(examples['explanation'])\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds_s42 = dataset_train_test.map(preprocess_dataset_s42, batched=True)\n",
    "tokenized_ds_s42 = tokenized_ds_s42.remove_columns(dataset_train_test['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The following parameters were taken from the DREAM-FLUTE paper (only the number of epochs has been increased because the model is smaller)'''\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"D:\\Documents\\PoliTo\\Deep NLP\\Project\\S4-2Model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    seed=42,\n",
    "    #weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=8,\n",
    "    load_best_model_at_end=True,\n",
    "    #eval_accumulation_steps=8,\n",
    "    #fp16=True,\n",
    "    #push_to_hub=True,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-08,\n",
    "    lr_scheduler_type='linear'\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_s42,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds_s42[\"train\"],\n",
    "    eval_dataset=tokenized_ds_s42[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "del tokenized_ds_s42\n",
    "del model_s42\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System 4 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s41_path = \"D:\\Documents\\PoliTo\\Deep NLP\\Project\\S4-1Model\\checkpoint-24112\"\n",
    "model_s41 = AutoModelForSeq2SeqLM.from_pretrained(model_s41_path)\n",
    "\n",
    "model_s42_path = \"D:\\Documents\\PoliTo\\Deep NLP\\Project\\S4-2Model\\checkpoint-24112\"\n",
    "model_s42 = AutoModelForSeq2SeqLM.from_pretrained(model_s42_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Class encapsulating the two steps of System 4 (Classify, then Explain)'''\n",
    "class DREAM_FLUTE_System4 :\n",
    "    def __init__(self, tokenizer = None, model_s41_path = None, model_s42_path = None) -> None:\n",
    "        self.tokenizer = tokenizer if tokenizer is not None else AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "        self.model_s41 = AutoModelForSeq2SeqLM.from_pretrained(model_s41_path) if model_s41_path is not None else AutoModelForSeq2SeqLM.from_pretrained(\"YoanBOUTE/DREAM-FLUTE-S4-Classify\")\n",
    "        self.model_s42 = AutoModelForSeq2SeqLM.from_pretrained(model_s42_path) if model_s42_path is not None else AutoModelForSeq2SeqLM.from_pretrained(\"YoanBOUTE/DREAM-FLUTE-S4-Explain\")\n",
    "\n",
    "    '''Expected input for function : \"Premise : ... . Hypothesis : ... . Is there a contradiction or entailment between the premise and hypothesis ?\" \n",
    "    Or list of strings in this format'''\n",
    "    def prediction_pipeline(self, inputs) :\n",
    "        if isinstance(inputs, str) :\n",
    "            tok_input = self.tokenizer(inputs, return_tensors='pt').input_ids\n",
    "            output_model_1 = self.model_s41.generate(tok_input, max_new_tokens=100)\n",
    "            decoded_output_model_1 = \"Label : \" + self.tokenizer.decode(output_model_1[0], skip_special_tokens=True)\n",
    "            intermediate_input = inputs[:inputs.find(\"Is there a contradiction or entailment between the premise and hypothesis ?\")] + decoded_output_model_1 + \". What is the explanation of the label associated to the premise and the hypothesis ?\"\n",
    "            tok_intermediate_input = self.tokenizer(intermediate_input, return_tensors='pt').input_ids\n",
    "            output_model_2 = self.model_s42.generate(tok_intermediate_input, max_new_tokens=100)\n",
    "\n",
    "            return decoded_output_model_1 + \". Explanation : \" + self.tokenizer.decode(output_model_2[0], skip_special_tokens=True)\n",
    "        \n",
    "        elif isinstance(inputs, list) and all(isinstance(input, str) for input in inputs) :\n",
    "            predictions = []\n",
    "            for input in inputs :\n",
    "                predictions.append(self.prediction_pipeline(input))\n",
    "            \n",
    "            return predictions\n",
    "        \n",
    "        else :\n",
    "            raise TypeError('Inputs should be either a list of two strings or a list of lists of two strings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ensemble class that loads all models from HuggingFace (or from the device if a path to the model is indicated) \n",
    "and implements the ensembling algorithm given in the DREAM-FLUTE paper'''\n",
    "class DREAM_FLUTE_Ensemble :\n",
    "    def __init__(self, tokenizer_path = None, s1_path = None, s2_path = None,\n",
    "                 s3_emo_path = None, s3_mot_path = None, s3_cons_path = None,\n",
    "                 s3_rot_path = None, s3_alldims_path = None, s4_clas_path = None, \n",
    "                 s4_exp_path = None, dream_path = None) -> None:\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path) if tokenizer_path is not None else AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "        self.model_s1 = AutoModelForSeq2SeqLM.from_pretrained(s1_path) if s1_path is not None else AutoModelForSeq2SeqLM.from_pretrained(\"YoanBOUTE/DREAM-FLUTE-S1\")\n",
    "        self.model_s2 = AutoModelForSeq2SeqLM.from_pretrained(s2_path) if s2_path is not None else AutoModelForSeq2SeqLM.from_pretrained(\"YoanBOUTE/DREAM-FLUTE-S2\")\n",
    "        self.model_s3_emo = AutoModelForSeq2SeqLM.from_pretrained(s3_emo_path) if s3_emo_path is not None else AutoModelForSeq2SeqLM.from_pretrained(\"YoanBOUTE/DREAM-FLUTE-S3-Emotion\")\n",
    "        self.model_s3_mot = AutoModelForSeq2SeqLM.from_pretrained(s3_mot_path) if s3_mot_path is not None else AutoModelForSeq2SeqLM.from_pretrained(\"YoanBOUTE/DREAM-FLUTE-S3-Motivation\")\n",
    "        self.model_s3_cons = AutoModelForSeq2SeqLM.from_pretrained(s3_cons_path) if s3_cons_path is not None else AutoModelForSeq2SeqLM.from_pretrained(\"YoanBOUTE/DREAM-FLUTE-S3-Consequence\")\n",
    "        self.model_s3_rot = AutoModelForSeq2SeqLM.from_pretrained(s3_rot_path) if s3_rot_path is not None else AutoModelForSeq2SeqLM.from_pretrained(\"YoanBOUTE/DREAM-FLUTE-S3-ROT\")\n",
    "        self.model_s3_alldims = AutoModelForSeq2SeqLM.from_pretrained(s3_alldims_path) if s3_alldims_path is not None else AutoModelForSeq2SeqLM.from_pretrained(\"YoanBOUTE/DREAM-FLUTE-S3-AllDims\")\n",
    "        self.model_s4 = DREAM_FLUTE_System4(self.tokenizer, s4_clas_path, s4_exp_path)\n",
    "        self.model_dream = AutoModelForSeq2SeqLM.from_pretrained(dream_path) if dream_path is not None else AutoModelForSeq2SeqLM.from_pretrained(\"RicoBorra/DREAM-t5-small\")\n",
    "    \n",
    "    '''Tokenizes the input, then feeds it to the given model, and decodes the output to have a string as result.\n",
    "    This method is callable for all models except System 4 (Use the method defined in the class of System 4)'''\n",
    "    def _prediction_pipeline(self, input : str, model) -> str :\n",
    "        tokenized_input = self.tokenizer(input, return_tensors='pt').input_ids\n",
    "        model_output = model.generate(tokenized_input, max_new_tokens=100)\n",
    "        decoded_output = self.tokenizer.decode(model_output[0], skip_special_tokens=True)\n",
    "        return decoded_output\n",
    "    \n",
    "    '''Preprocesses the input for each model, then feeds it to the pipeline.\n",
    "    Returns a dictionary of all models' predictions.'''\n",
    "    def _get_all_predictions(self, input : list) :\n",
    "        prem, hyp = input \n",
    "        prem = prem.strip()\n",
    "        hyp = hyp.strip()\n",
    "        if not prem.endswith('.') :\n",
    "            prem += '.'\n",
    "        if not hyp.endswith('.') :\n",
    "            hyp += '.' \n",
    "\n",
    "        predictions = dict()\n",
    "\n",
    "        input_1 = f\"Premise : {prem} Hypothesis : {hyp} Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions['S1'] = self._prediction_pipeline(input_1, self.model_s1)\n",
    "\n",
    "        input_2 = f\"Premise : {prem} Hypothesis : {hyp} What is the type of figurative language involved? Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions['S2'] = self._prediction_pipeline(input_2, self.model_s2)\n",
    "\n",
    "        # DREAM elaborations for system 3\n",
    "        input_dream_prem = f\"[SITUATION] {prem} [QUERY] \"\n",
    "        input_dream_hyp = f\"[SITUATION] {hyp} [QUERY] \"\n",
    "        prem_elaborations = {key : self._prediction_pipeline(input_dream_prem + key, self.model_dream) for key in ['emotion', 'motivation', 'consequence', 'rot']}\n",
    "        for key, elab in prem_elaborations.items() :\n",
    "            elab = elab.strip()\n",
    "            if not elab.endswith('.') :\n",
    "                prem_elaborations[key] += '.' \n",
    "        hyp_elaborations = {key : self._prediction_pipeline(input_dream_hyp + key, self.model_dream) for key in ['emotion', 'motivation', 'consequence', 'rot']}\n",
    "        for key, elab in hyp_elaborations.items() :\n",
    "            elab = elab.strip()\n",
    "            if not elab.endswith('.') :\n",
    "                hyp_elaborations[key] += '.' \n",
    "\n",
    "        input_3_emo = f\"Premise : {prem} [Emotion] {prem_elaborations['emotion']} Hypothesis : {hyp} [Emotion] {hyp_elaborations['emotion']} Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions['S3-emo'] = self._prediction_pipeline(input_3_emo, self.model_s3_emo)\n",
    "\n",
    "        input_3_mot = f\"Premise : {prem} [Motivation] {prem_elaborations['motivation']} Hypothesis : {hyp} [Motivation] {hyp_elaborations['motivation']} Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions['S3-mot'] = self._prediction_pipeline(input_3_mot, self.model_s3_mot)\n",
    "\n",
    "        input_3_cons = f\"Premise : {prem} [Consequence] {prem_elaborations['consequence']} Hypothesis : {hyp} [Consequence] {hyp_elaborations['consequence']} Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions['S3-cons'] = self._prediction_pipeline(input_3_cons, self.model_s3_cons)\n",
    "\n",
    "        input_3_rot = f\"Premise : {prem} [Rot] {prem_elaborations['rot']} Hypothesis : {hyp} [Rot] {hyp_elaborations['rot']} Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions['S3-rot'] = self._prediction_pipeline(input_3_rot, self.model_s3_rot)\n",
    "\n",
    "        input_3_all = f\"Premise : {prem} \"\n",
    "        for key, elab in prem_elaborations.items() :\n",
    "            input_3_all += f\"[{key.capitalize()}] {elab} \"\n",
    "        input_3_all += f\"Hypothesis : {hyp} \"\n",
    "        for key, elab in hyp_elaborations.items() :\n",
    "            input_3_all += f\"[{key.capitalize()}] {elab} \"\n",
    "        input_3_all += \"Is there a contradiction or entailment between the premise and hypothesis ?\"\n",
    "        predictions['S3-all'] = self._prediction_pipeline(input_3_all, self.model_s3_alldims)\n",
    "\n",
    "        # The input for system 4 is in the same format as for system 1\n",
    "        predictions['S4'] = self.model_s4.prediction_pipeline(input_1)\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    '''Uses the predictions from each model to compute the final prediction of the ensemble'''\n",
    "    def _ensemble_algorithm(self, model_outputs) :\n",
    "        # Firstly, the label is selected based on the majority between the 5 best models (according to the paper : systems 1, 2, 3-motivation, 3-alldims, 4)\n",
    "        labels = [model_outputs[key].split('.')[0] for key in ['S1', 'S2', 'S3-mot', 'S3-all', 'S4']]\n",
    "        # Sometimes, it might happen with the small models that the generated label is a mix of words, like 'Contratailment' or 'Endiction'\n",
    "        for label in labels :\n",
    "            if label not in ['Label : Contradiction', 'Label : Entailment'] :\n",
    "                labels.remove(label)\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        ix = np.argmax(counts)\n",
    "        major_label = unique[ix]\n",
    "\n",
    "        # Then, pick the explanation of the first system agreeing with the majority label, following an order indicated in the paper\n",
    "        for key in ['S3-cons', 'S3-emo', 'S2', 'S3-all', 'S3-mot', 'S4', 'S1'] :\n",
    "            substrings = model_outputs[key].split('.')\n",
    "            label = substrings[0]\n",
    "            explanation = substrings[1]\n",
    "\n",
    "            if label == major_label :\n",
    "                break\n",
    "\n",
    "        return major_label + '.' + explanation + '.'\n",
    "\n",
    "    '''Expected input : [Premise_sentence, hypothesis_sentence] or list of inputs'''\n",
    "    def predict(self, inputs) :\n",
    "        if isinstance(inputs, list) and all(isinstance(input, str) for input in inputs) and len(inputs) == 2 :\n",
    "            preds = self._get_all_predictions(inputs)\n",
    "            final_pred = self._ensemble_algorithm(preds)\n",
    "        \n",
    "            return final_pred \n",
    "        \n",
    "        elif isinstance(inputs, list) and all(isinstance(input, list) for input in inputs) :\n",
    "            predictions = []\n",
    "            for input in inputs :\n",
    "                predictions.append(self.predict(input))\n",
    "            \n",
    "            return predictions\n",
    "        else :\n",
    "            raise TypeError('Inputs should be either a list of two strings or a list of lists of two strings')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
