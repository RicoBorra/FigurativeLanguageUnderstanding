{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "import nltk\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "NUM_EPOCHS = 8\n",
    "base_checkpoint = \"t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(base_checkpoint)\n",
    "\n",
    "def add_cols(entry):\n",
    "\n",
    "    premise = entry[\"premise\"].strip()\n",
    "    hypothesis = entry[\"hypothesis\"].strip()\n",
    "\n",
    "    if not premise.endswith(\".\"):\n",
    "        premise += \".\"\n",
    "    assert(premise.endswith(\".\"))\n",
    "    if not hypothesis.endswith(\".\"):\n",
    "        hypothesis += \".\"\n",
    "    assert(hypothesis.endswith(\".\"))\n",
    "\n",
    "    # Columns for System 1\n",
    "    entry[\"premise_hypothesis\"] = 'Premise: ' + premise + ' Hypothesis: ' + hypothesis + ' Is there a contradiction or entailment between the premise and hypothesis ?'\n",
    "    #entry[\"label_explanation\"] = 'Explanation: ' + entry[\"explanation\"] + '. Label: ' + entry[\"label\"]\n",
    "    entry[\"label_explanation\"] = 'Label: ' + entry[\"label\"] + '. Explanation: ' + entry[\"explanation\"]\n",
    "    return entry\n",
    "\n",
    "df = pd.read_csv(\"complete_dataset.csv\").fillna(\"\")\n",
    "df_syn = pd.read_csv(\"synthetic_data_merge.tsv\", sep=\"\\t\").fillna(\"\")\n",
    "ds = Dataset.from_pandas(df).shuffle(seed=42)\n",
    "ds_syn = Dataset.from_pandas(df_syn).shuffle(seed=42)\n",
    "\n",
    "ds = ds.map(add_cols)\n",
    "ds_syn = ds_syn.map(add_cols)\n",
    "\n",
    "def preprocess_dataset_s1(examples):\n",
    "    model_inputs = tokenizer(examples['premise_hypothesis'], truncation=True, max_length=512)\n",
    "    labels = tokenizer(examples['label_explanation'], truncation=True, max_length=512)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"translate English to French: Hello big guy, I'm a very strange man\"\n",
    "tokenizer.decode(model.generate(tokenizer(sentence, return_tensors=\"pt\")['input_ids'])[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for now_ds in (ds_syn.train_test_split(test_size=0.2), ds.train_test_split(test_size=0.2)):\n",
    "\n",
    "    curr_ds = now_ds.map(preprocess_dataset_s1, batched=True).remove_columns(now_ds['train'].column_names)\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"T5-small-synthetic-FLUTE\",\n",
    "        learning_rate=3e-4,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=2*8,\n",
    "        save_total_limit=2,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        report_to=\"none\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        eval_accumulation_steps=1,\n",
    "        logging_steps=1,\n",
    "        lr_scheduler_type=\"constant\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=curr_ds[\"train\"],\n",
    "        eval_dataset=curr_ds[\"test\"].select(range(350)),\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # have to do batched rouge computation otherwise not enough memory\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    metrics = {'rouge1': 0., 'rouge2': 0., 'rougeL': 0., 'rougeLsum': 0.}\n",
    "    count = 0\n",
    "    for i in range(0, len(curr_ds['test']), 80):\n",
    "        count += 1\n",
    "        (predictions, _), label_ids, _ = trainer.predict(test_dataset=curr_ds['test'].select(range(i, min(i+80, len(curr_ds['test'])))))\n",
    "        # delete stuff after EOS token\n",
    "        predicted_token_ids = torch.argmax(torch.from_numpy(predictions), dim=-1)\n",
    "        for i in range(predicted_token_ids.shape[0]):\n",
    "            ind = (predicted_token_ids[i] == 1).nonzero(as_tuple=True)[0]\n",
    "            if ind.numel() != 0:\n",
    "                predicted_token_ids[i, ind[0]:] = 1\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(predicted_token_ids, skip_special_tokens=True)\n",
    "        labels = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        new_metrics = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        for k in new_metrics:\n",
    "            metrics[k] += new_metrics[k]\n",
    "\n",
    "    for k in metrics:\n",
    "            metrics[k] /= count\n",
    "\n",
    "    print(metrics['rouge1'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curr_ds = now_ds.map(preprocess_dataset_s1, batched=True).remove_columns(now_ds['train'].column_names)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(base_checkpoint)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"synthetics\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=2*BATCH_SIZE,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=8,\n",
    "    report_to=\"none\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_accumulation_steps=1,\n",
    "    logging_steps=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=curr_ds[\"train\"],\n",
    "    eval_dataset=curr_ds[\"test\"].select(range(350)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# have to do batched rouge computation otherwise not enough memory\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "metrics = {'rouge1': 0., 'rouge2': 0., 'rougeL': 0., 'rougeLsum': 0.}\n",
    "count = 0\n",
    "for i in range(0, len(curr_ds['test']), 100):\n",
    "    count += 1\n",
    "    (predictions, _), label_ids, _ = trainer.predict(test_dataset=curr_ds['test'].select(range(i, min(i+100, len(curr_ds['test'])))))\n",
    "    predicted_token_ids = torch.argmax(torch.from_numpy(predictions), dim=-1)\n",
    "    # delete stuff after EOS token\n",
    "    for i in range(predicted_token_ids.shape[0]):\n",
    "        ind = (predicted_token_ids[i] == 1).nonzero(as_tuple=True)[0]\n",
    "        if ind.numel() != 0:\n",
    "            predicted_token_ids[i, ind[0]:] = 1\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predicted_token_ids, skip_special_tokens=True)\n",
    "    labels = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    new_metrics = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    for k in new_metrics:\n",
    "        metrics[k] += new_metrics[k]\n",
    "\n",
    "for k in metrics:\n",
    "        metrics[k] /= count\n",
    "\n",
    "print(metrics['rouge1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
